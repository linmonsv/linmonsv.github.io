<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"always","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Water&#39;s Home">
<meta property="og:url" content="http://example.com/page/13/index.html">
<meta property="og:site_name" content="Water&#39;s Home">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Water">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/page/13/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Water's Home</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Water's Home</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Just another Life Style</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2019/04/02/12-support-vector-machines/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Water">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Water's Home">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/04/02/12-support-vector-machines/" class="post-title-link" itemprop="url">12 Support Vector Machines</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-04-02 16:07:38" itemprop="dateCreated datePublished" datetime="2019-04-02T16:07:38+08:00">2019-04-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-07-04 15:26:09" itemprop="dateModified" datetime="2022-07-04T15:26:09+08:00">2022-07-04</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/machine-learning/" itemprop="url" rel="index"><span itemprop="name">machine-learning</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/machine-learning/Machine-Learning-Offered-By-Stanford-University/" itemprop="url" rel="index"><span itemprop="name">Machine Learning Offered By Stanford University</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Optimization-Objective"><a href="#Optimization-Objective" class="headerlink" title="Optimization Objective"></a>Optimization Objective</h2><p><strong>Within supervised learning, the performance of many supervised learning algorithms will be pretty similar</strong> and when that is less more often be whether you use learning algorithm A or learning algorithm B but when that is small there will often be things like the amount of data you are creating these algorithms on. That’s always <strong>your skill in applying this algorithms</strong>. Seems like your <strong>choice of the features</strong> that you designed to give the learning algorithms and how you choose the <strong>regularization parameter</strong> and things like that. <strong>Support Vector Machine</strong> (<strong>SVM</strong>) : sometimes gives a cleaner and sometimes more powerful way of learning complex nonlinear functions. <strong>Alternative view of logistic regression</strong> [latex]h_\theta (x) &#x3D; \frac {1}{1 + e^{-\theta ^{T}x}}[&#x2F;latex]   Cost of example :  [latex]-(ylogh_\theta (x) + (1-y)log(1-h_\theta(x))) [&#x2F;latex]   [latex]&#x3D; -ylog\frac {1}{1 + e^{-\theta ^{T}x}} + (1-y)log(1-\frac {1}{1 + e^{-\theta ^{T}x}})[&#x2F;latex]   <strong>Support vector machine</strong> Logistic regression :  [latex]\underset{\theta }{min} \frac{1}{m} [\sum_{i&#x3D;1}^{m}y^{(i)}(-logh_\theta(x^{(i)})) + (1-y^{(i)})(-log(1-h_\theta (x^{(i)})))] + \frac{\lambda }{2m}\sum_{j&#x3D;1}^{n} \theta _j^2[&#x2F;latex]   Support vector machine :  [latex]\underset{\theta }{min} C [\sum_{i&#x3D;1}^{m}y^{(i)}cost_1(\theta ^Tx^{(i)}) + (1-y^{(i)})cost_0(\theta ^Tx^{(i)})] + \frac{1}{2}\sum_{i&#x3D;1}^{n} \theta _j^2[&#x2F;latex]  </p>
<h2 id="Large-Margin-Intuition"><a href="#Large-Margin-Intuition" class="headerlink" title="Large Margin Intuition"></a>Large Margin Intuition</h2><p>Sometimes people talk about support vector machines, as <strong>large margin classifiers</strong>. The margin of the support vector machine and this gives the SVM a certain robustness, because it tries to separate the data with as a large a margin as possible. So the support vector machine is sometimes also called a large  margin classifier. [latex]min \frac{1}{2} \sum _{j&#x3D;1}^{n} \ \theta _j^2 \ s.t \ \left\{\begin{matrix} \theta^Tx^{(i)} \geq \ 1 \ \ if \ y^{(i)} &#x3D; 1\\ \theta^Tx^{(i)} \leq -1 \ \ if \ y^{(i)} &#x3D; 0\\ \end{matrix}\right.[&#x2F;latex]   In practice when applying support vector machines, when C is not very very large like that, it can do a better job ignoring the few outliers.</p>
<h2 id="Mathematics-Behind-Large-Margin-Classification"><a href="#Mathematics-Behind-Large-Margin-Classification" class="headerlink" title="Mathematics Behind Large Margin Classification"></a>Mathematics Behind Large Margin Classification</h2><p>[latex]\left \ u \right \ &#x3D; \sqrt{u_1^2 + u_2^2}[&#x2F;latex] <strong>p</strong> is the length of the projection of the vector V onto the vector U. so [latex]u^Tv &#x3D; p \cdot \left \ u \right \[&#x2F;latex] and [latex]u^Tv &#x3D; u_1 \times  v_1 + u_2 \times v_2[&#x2F;latex] so [latex]p \cdot \left \ u \right \ &#x3D; u_1 \times  v_1 + u_2 \times v_2[&#x2F;latex] <strong>SVM Decision Boundary</strong> [latex]\underset {\theta }{min} \frac{1}{2} \sum _{j&#x3D;1}^{n} \ \theta _j^2 \ s.t \ \left\{\begin{matrix} \theta^Tx^{(i)} \geq \ 1 \ \ if \ y^{(i)} &#x3D; 1\\ \theta^Tx^{(i)} \leq -1 \ \ if \ y^{(i)} &#x3D; 0\\ \end{matrix}\right.[&#x2F;latex]   <strong>if n &#x3D;&#x3D; 2:</strong> [latex]\begin{Vmatrix} u \end{Vmatrix} &#x3D; \sqrt{u_1^2 + u_2^2}[&#x2F;latex]   [latex]\frac{1}{2} (\theta _1^2 + \theta _2^2) &#x3D; \frac{1}{2} (\sqrt {\theta _1^2 + \theta _2^2})^2 &#x3D; \frac{1}{2}\left \ \theta \right \ ^2[&#x2F;latex]   <strong>So all the support vector machine is doing in the optimization objective is it’s minimizing the squared norm of the square length of the parameter vector theta.</strong> [latex]p^{(i)}[&#x2F;latex] : a projection of the i-th training example onto the parameter vector [latex]\theta [&#x2F;latex]. [latex]\theta ^Tx^{(i)} &#x3D; \theta_1 \cdot x_1^{(i)} + \theta_2 \cdot x_2^{(i)} [&#x2F;latex]   <strong>SVM Decision Boundary</strong> [latex]\underset {\theta }{min} \frac{1}{2} \sum _{j&#x3D;1}^{n} \ \theta _j^2 \ s.t \ \left\{\begin{matrix} p^{(i)} \cdot \left \ \theta \right \ \geq \ 1 \ \ if \ y^{(i)} &#x3D; 1\\ p^{(i)} \cdot \left \ \theta \right \ \leq -1 \ \ if \ y^{(i)} &#x3D; 0\\ \end{matrix}\right.[&#x2F;latex] If we can make the norm of theta smaller and therefore make the squared norm of theta smaller, <strong>which is why the SVM would choose this hypothesis</strong> on the right instead. And this is how the SVM gives rise to this large margin certification effect. Mainly, if you look at this green line, if you look at this green hypothesis we want the projections of my positive and negative examples onto theta to be large, and the only way for that to hold true this is if surrounding the green line. There’s this large margin, there’s this large gap that separates positive and negative examples is really the magnitude of this gap. The magnitude of this margin is exactly the values of P1, P2, P3 and so on. And so by making the margin large, by these tyros P1, P2, P3 and so on that’s <strong>the SVM can end up with a smaller value for the norm of theta</strong> which is what it is trying to do in the objective. And <strong>this is why this machine ends up with enlarge margin classifiers</strong> because itss trying to maximize the norm of these P1 which is the distance from the training examples to the decision boundary.</p>
<h2 id="Kernels"><a href="#Kernels" class="headerlink" title="Kernels"></a>Kernels</h2><ul>
<li>complex polynomial features</li>
</ul>
<p>[latex]\theta_0 + \theta_1x_1 + \theta_2x_2 + \theta_3x_1x_2 + \theta_4x_1^2 + \theta_5x_2^2 + \cdots[&#x2F;latex]  </p>
<ul>
<li>new denotation</li>
</ul>
<p>[latex]f_1 &#x3D; x_1, \ f_2 &#x3D; x_2, \ f_3 &#x3D; x_1x_2, \ f_4 &#x3D; x_1^2, \ f_5 &#x3D; x_2^2[&#x2F;latex] [latex]h_\theta (x) &#x3D; \theta_1f_1 + \theta_2f_2 + \cdots + \theta_nf_n[&#x2F;latex]   <strong>Gaussian Kernel</strong> [latex]f_1 &#x3D; similarity(x, l^{(1)}) &#x3D; e^{(-\frac {\left \ x - l^{(1)} \right \ ^ 2}{2\sigma ^2})}[&#x2F;latex]   [latex]\left \ x - l^{(1)} \right \ ^ 2 &#x3D; \sum _{j&#x3D;1}^{n}(x_j - l_j^{(1)})^2[&#x2F;latex]   We define some extra features using landmarks and similarity functions to learn more complex nonlinear classifiers. We <strong>use new features</strong> that are computed by Kernels, <strong>not original features</strong>.</p>
<h4 id="How-the-landmarks-are-chosen"><a href="#How-the-landmarks-are-chosen" class="headerlink" title="How the landmarks are chosen"></a>How the landmarks are chosen</h4><p>Choose the the location of my landmarks to be exactly near the locations of my m training examples. Given [latex](x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \cdots , (x^{(m)}, y^{(m)})[&#x2F;latex] choose [latex]l^{(1)} &#x3D; x^{(1)}, l^{(2)} &#x3D; x^{(2)}, \cdots \cdots , l^{(m)} &#x3D; x^{(m)}[&#x2F;latex] Given example x :  [latex]f_1 &#x3D; similarity(x, l^{(1)})[&#x2F;latex] [latex]f_2 &#x3D; similarity(x, l^{(2)})[&#x2F;latex] [latex]\cdots [&#x2F;latex] **Cost Function : ** [latex]minC\sum _{i&#x3D;1}^{m}[y^{(i)}cost_1(\theta^Tf^{(i)}) + (1-y^{(i)})cost_0(\theta^Tf^{(i)})]+\frac{1}{2}\sum_{j&#x3D;1}^{n&#x3D;m}\theta_j^2[&#x2F;latex]   Using kernels with logistic regression is going too very slow.</p>
<h4 id="How-to-choose-latex-C-and-sigma-x2F-latex"><a href="#How-to-choose-latex-C-and-sigma-x2F-latex" class="headerlink" title="How to choose [latex]C \ and \ \sigma[&#x2F;latex]"></a>How to choose [latex]C \ and \ \sigma[&#x2F;latex]</h4><p>[latex]C &#x3D; 1 \ &#x2F; \ \lambda[&#x2F;latex]</p>
<ul>
<li>[latex]C[&#x2F;latex] big: overfitting, higher variance</li>
<li>[latex]C[&#x2F;latex] small: underfitting, higher bias</li>
<li>[latex]\sigma[&#x2F;latex] big : lower variance, higher bias</li>
<li>[latex]\sigma[&#x2F;latex] small : lower bias, higher variance</li>
</ul>
<h2 id="Using-An-SVM"><a href="#Using-An-SVM" class="headerlink" title="Using An SVM"></a>Using An SVM</h2><p><strong>Some library function :</strong></p>
<ul>
<li>liblinear</li>
<li>libsvm</li>
</ul>
<p><strong>There are a few things need to do :</strong></p>
<ol>
<li>parameter’s C</li>
<li>choose the kernel (If we decide not to use any kernel. And the idea of no kernel is also called a <strong>linear kernel</strong>)</li>
</ol>
<h4 id="Logistic-Regression-or-Support-Vector-Machine"><a href="#Logistic-Regression-or-Support-Vector-Machine" class="headerlink" title="Logistic Regression or Support Vector Machine"></a>Logistic Regression or Support Vector Machine</h4><ul>
<li>n : number of features</li>
<li>m : number of training examples</li>
</ul>
<ol>
<li>n &gt;&gt; m, Logistic Regression or Linear Kernel</li>
<li>n small, and m middle, like 1 &lt; n &lt; 1000, 10 &lt; m &lt; 10000, SVM with Gaussian Kernel</li>
<li>n small, and m big, like 1 &lt; n &lt; 1000, 50000 &lt; m, SVM is slower, so try to munually create more features and then use logistic regression or an SVM without the Kernel</li>
</ol>
<p>Well for all of these problems, for all of these different regimes, a well designed <strong>neural network</strong> is likely to work well as well. The <strong>algorithm</strong> does matter, but what often matters even more is things like, <strong>how much data</strong> do you have. And <strong>how skilled</strong> are you, how good are you at doing <strong>error analysis and debugging learning algorithms</strong>, figuring out how to <strong>design new features</strong> and <strong>figuring out what other features</strong> to give you learning algorithms and so on.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2019/04/01/11-machine-learning-system-design/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Water">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Water's Home">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/04/01/11-machine-learning-system-design/" class="post-title-link" itemprop="url">11 Machine Learning System Design</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-04-01 16:07:51" itemprop="dateCreated datePublished" datetime="2019-04-01T16:07:51+08:00">2019-04-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-07-04 15:26:09" itemprop="dateModified" datetime="2022-07-04T15:26:09+08:00">2022-07-04</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/machine-learning/" itemprop="url" rel="index"><span itemprop="name">machine-learning</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/machine-learning/Machine-Learning-Offered-By-Stanford-University/" itemprop="url" rel="index"><span itemprop="name">Machine Learning Offered By Stanford University</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Prioritizing-What-to-Work-On"><a href="#Prioritizing-What-to-Work-On" class="headerlink" title="Prioritizing What to Work On"></a>Prioritizing What to Work On</h2><p>How to strategize putting together a complex machine learning system. It’s hard to choose the options which is the best use of your time. In fact, if you even get to the stage where you brainstorm a list of different options to try, you’re probably already ahead of the curve. We must have a more <strong>systematic way to choose</strong> among the options of the many different things.</p>
<h2 id="Error-Analysis"><a href="#Error-Analysis" class="headerlink" title="Error Analysis"></a>Error Analysis</h2><p>If you’re starting work on a machine learning product or building a machine learning application, it is often considered very good practice to start, not by building a very complicated system with lots of complex features and so on, <strong>but to instead start by building a very simple algorithm, the you can implement quickly</strong>. It’s often by implementing even a very, very quick and dirty implementation and by plotting learning curves that that helps you make these decisions. And often by doing that, this is the process that would inspire you to design new features. Or they’ll tell you whether the current things or current shortcomings of the system and give you <strong>the inspiration</strong> you need to come up with improvements to it.</p>
<h4 id="Recommended-way"><a href="#Recommended-way" class="headerlink" title="Recommended way :"></a>Recommended way :</h4><ol>
<li>start by building a very simple algorithm</li>
<li>plot a learning curve</li>
<li>error analysis (a single rule number evaluation metric)</li>
</ol>
<p><strong>Strongly recommended way to do error analysis is on the cross validation set rather than the test set.</strong></p>
<h2 id="Error-Metrics-for-Skewed-Classes"><a href="#Error-Metrics-for-Skewed-Classes" class="headerlink" title="Error Metrics for Skewed Classes"></a>Error Metrics for Skewed Classes</h2><p>It’s particularly tricky to come up with an appropriate error metric, or evaluation metric, for your learning algorithm. [table id&#x3D;1 &#x2F;]</p>
<h2 id="Trading-Off-Precision-and-Recall"><a href="#Trading-Off-Precision-and-Recall" class="headerlink" title="Trading Off Precision and Recall"></a>Trading Off Precision and Recall</h2><p>[latex]Precision &#x3D; TP &#x2F; (TP + FP)[&#x2F;latex]</p>
<ul>
<li>If you want to make predicting only when you’re <strong>more confident</strong>, and so you end up with a classifier that has <strong>higher precision</strong>.</li>
</ul>
<p>[latex]Recall&#x3D; TP &#x2F; (TP + FN)[&#x2F;latex]</p>
<ul>
<li>If we want to <strong>avoid missing</strong> too many actual cases. So we want to avoid the false negatives. And in this case, what we would have is going to be a <strong>higher recall</strong> classifier.</li>
</ul>
<p>[latex]F_1Score : 2 \frac{PR}{P + R}[&#x2F;latex]</p>
<ul>
<li>Maybe we can choose the higher F1 value in some cases.</li>
</ul>
<h2 id="Data-For-Machine-Learning"><a href="#Data-For-Machine-Learning" class="headerlink" title="Data For Machine Learning"></a>Data For Machine Learning</h2><p>In <strong>some cases</strong>, I had cautioned against blindly going out and just spending lots of time collecting <strong>lots of data</strong>, because <strong>it’s only</strong> sometimes that that would <strong>actually help</strong>. In machine learning that often in machine learning it’s not who has the best algorithm that wins, it’s who has the <strong>most data</strong>. If you have a lot of data and you train a learning algorithm with lot of parameters, that might be a good way to give a high performance learning algorithm.</p>
<h4 id="The-Key"><a href="#The-Key" class="headerlink" title="The Key :"></a>The Key :</h4><ol>
<li>Find some features x and confidently predict the value of y.</li>
<li>Actually get a large training set, and train the learning algorithm with a lot of parameters in the training set.</li>
</ol>
<p><strong>If you can’t do both then you need a very kind performance learning algorithm.</strong></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2019/03/27/precision-recall/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Water">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Water's Home">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/03/27/precision-recall/" class="post-title-link" itemprop="url">Precision & Recall</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-03-27 16:13:55" itemprop="dateCreated datePublished" datetime="2019-03-27T16:13:55+08:00">2019-03-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-07-04 15:26:09" itemprop="dateModified" datetime="2022-07-04T15:26:09+08:00">2022-07-04</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/uncategorized/" itemprop="url" rel="index"><span itemprop="name">uncategorized</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>[[“Matrix”,”#colspan#”,”Prediction Value”,”#colspan#”],[“”,””,”Positive”,”Negtive”],[“Actual Value”,”Negtive”,”FP”,”TN”],[“#rowspan#”,”Positive”,”TP”,”FN”]]</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2019/03/27/10-advice-for-applying-machine-learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Water">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Water's Home">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/03/27/10-advice-for-applying-machine-learning/" class="post-title-link" itemprop="url">10 Advice for Applying Machine Learning</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-03-27 15:31:08" itemprop="dateCreated datePublished" datetime="2019-03-27T15:31:08+08:00">2019-03-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-07-04 15:26:09" itemprop="dateModified" datetime="2022-07-04T15:26:09+08:00">2022-07-04</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/machine-learning/" itemprop="url" rel="index"><span itemprop="name">machine-learning</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/machine-learning/Machine-Learning-Offered-By-Stanford-University/" itemprop="url" rel="index"><span itemprop="name">Machine Learning Offered By Stanford University</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Deciding-What-to-Try-Next"><a href="#Deciding-What-to-Try-Next" class="headerlink" title="Deciding What to Try Next"></a>Deciding What to Try Next</h2><p>If you are <strong>developing</strong> a machine learning system or trying to <strong>improve the performance</strong> of a machine learning system, how do you go about deciding <strong>what are the proxy avenues to try next</strong>? If you find that this is making <strong>huge errors in this prediction</strong>. <strong>What should you then try</strong> mixing in order to improve the learning algorithm? One thing they could try, is to get <strong>more training examples</strong>. But sometimes getting more training data doesn’t actually help. Other things you might try are to well maybe try a <strong>smaller set of features</strong>. There is a pretty simple technique that can let you very <strong>quickly rule out half of the things on this list</strong> as being potentially promising things to pursue. And there is a very simple technique, that if you run, can easily rule out many of these options, and potentially save you a lot of time pursuing something that’s just is not going to work. <strong>Machine Learning Diagnostics</strong>, and what a diagnostic is, is a test you can run, to get insight into what is or isn’t working with an algorithm, and which will often give you insight as to what are promising things to try to improve a learning algorithm’s performance.</p>
<h2 id="Evaluating-a-Hypothesis"><a href="#Evaluating-a-Hypothesis" class="headerlink" title="Evaluating a Hypothesis"></a>Evaluating a Hypothesis</h2><p>If there is any sort of ordinary to the data. That should be better to send a <strong>random 70%</strong> of your data to the <strong>training set</strong> and a <strong>random 30%</strong> of your data to the <strong>test set</strong>.</p>
<h2 id="Model-Selection-and-Train-Validation-Test-Sets"><a href="#Model-Selection-and-Train-Validation-Test-Sets" class="headerlink" title="Model Selection and Train_Validation_Test Sets"></a>Model Selection and Train_Validation_Test Sets</h2><p>To send <strong>60%</strong> of your data’s, your training set, maybe <strong>20%</strong> to your cross validation set, and <strong>20%</strong> to your test set.</p>
<h2 id="Diagnosing-Bias-vs-Variance"><a href="#Diagnosing-Bias-vs-Variance" class="headerlink" title="Diagnosing Bias vs. Variance"></a>Diagnosing Bias vs. Variance</h2><p>The <strong>training set error, will be high</strong>. And you might find that the <strong>cross validation error will also be high</strong>. It might be a close. Maybe just slightly higher than a training error. The algorithm may be suffering from <strong>high bias</strong>. In contrast if your algorithm is suffering from <strong>high variance</strong>.</p>
<h2 id="Regularization-and-Bias-Variance"><a href="#Regularization-and-Bias-Variance" class="headerlink" title="Regularization and Bias_Variance"></a>Regularization and Bias_Variance</h2><p><strong>Looking at the plot of the whole or cross validation error</strong>, you can either manually, automatically try to select a point that minimizes  the cross-validation error and select the value of lambda corresponding to low cross-validation error.</p>
<h2 id="Learning-Curves"><a href="#Learning-Curves" class="headerlink" title="Learning Curves"></a>Learning Curves</h2><p>Learning curves is often a very useful thing to plot. If either you wanted to sanity check that your algorithm is working correctly, or if you want to improve the performance of the algorithm. To plot a learning curve, is plot j train which is, say, average squared error on my training set or Jcv which is the average squared error on my <strong>cross validation set</strong>. And I’m going to plot that as a function of m, that is as a function of <strong>the number of training examples</strong>. In the <strong>high variance</strong> setting, getting <strong>more training data</strong> is, indeed, likely to help.</p>
<h2 id="Deciding-What-to-Do-Next-Revisited"><a href="#Deciding-What-to-Do-Next-Revisited" class="headerlink" title="Deciding What to Do Next Revisited"></a>Deciding What to Do Next Revisited</h2><ol>
<li>getting more training examples is good for <em>high variance</em></li>
<li>a smaller set of features fixes <em>high variance</em></li>
<li>adding features usually is a solution for fixing high bias</li>
<li>similarly, adding polynomial features</li>
<li>decreasing lambda fixes fixes high bias</li>
<li>increasing lambda fixes <em>high variance</em></li>
</ol>
<p>It turns out if you’re applying neural network very often using <strong>a large neural network</strong> often it’s actually the larger, the better. <strong>Using a single hidden layer is a reasonable default</strong>, but if you want to choose the number of hidden layers, one other thing you can try is find yourself a training cross-validation, and test set split and try training neural networks with <strong>one hidden layer or two hidden layers or three hidden layers</strong> and see which of those neural networks performs best on the cross-validation sets.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2019/03/27/9-neural-networks-learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Water">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Water's Home">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/03/27/9-neural-networks-learning/" class="post-title-link" itemprop="url">9 Neural Networks: Learning</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-03-27 14:41:20" itemprop="dateCreated datePublished" datetime="2019-03-27T14:41:20+08:00">2019-03-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-07-04 15:26:09" itemprop="dateModified" datetime="2022-07-04T15:26:09+08:00">2022-07-04</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/machine-learning/" itemprop="url" rel="index"><span itemprop="name">machine-learning</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/machine-learning/Machine-Learning-Offered-By-Stanford-University/" itemprop="url" rel="index"><span itemprop="name">Machine Learning Offered By Stanford University</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Cost-Function"><a href="#Cost-Function" class="headerlink" title="Cost Function"></a>Cost Function</h2><p>Two types of classification problems:</p>
<ul>
<li>Binary classification : where the labels y are either zero or one.</li>
<li>multiclass classification : where we may have k distinct classes.</li>
</ul>
<h2 id="Backpropagation-Algorithm"><a href="#Backpropagation-Algorithm" class="headerlink" title="Backpropagation Algorithm"></a>Backpropagation Algorithm</h2><p>It’s too hard to describe.</p>
<h2 id="Backpropagation-Intuition"><a href="#Backpropagation-Intuition" class="headerlink" title="Backpropagation Intuition"></a>Backpropagation Intuition</h2><p>It’s too hard to describe.</p>
<h2 id="Implementation-Note-Unrolling-Parameters"><a href="#Implementation-Note-Unrolling-Parameters" class="headerlink" title="Implementation Note_ Unrolling Parameters"></a>Implementation Note_ Unrolling Parameters</h2><p>It’s too hard to describe.</p>
<h2 id="Gradient-Checking"><a href="#Gradient-Checking" class="headerlink" title="Gradient Checking"></a>Gradient Checking</h2><p>Back prop as an algorithm has one unfortunate property is that there are many ways to have subtle bugs in back prop so that if you run it with gradient descent or some other optimization algorithm, it could actually look like it’s working. And, you know, your cost function [latex]J(\theta )[&#x2F;latex] may end up decreasing on every iteration of gradient descent, but this could pull through even though there might be some bug in your implementation of back prop. So it looks like [latex]J(\theta )[&#x2F;latex] is decreasing, but you might just wind up with a neural network that has a higher level of error than you would with a bug-free implementation and you might just not know that there was this subtle bug that’s giving you this performance. Gradient checking that eliminates almost all of these problems.</p>
<h2 id="Random-Initialization"><a href="#Random-Initialization" class="headerlink" title="Random Initialization"></a>Random Initialization</h2><p>To train a neural network, what you should do is randomly initialize the weights to, you know, small values close to 0, between [latex]-\epsilon[&#x2F;latex] and [latex]+\epsilon[&#x2F;latex].</p>
<h2 id="Putting-It-Together"><a href="#Putting-It-Together" class="headerlink" title="Putting It Together"></a>Putting It Together</h2><p>How to implement a neural network learning algorithm ?</p>
<ul>
<li>Pick some network architecture, it means connectivity pattern between the neurons.</li>
<li>Once you decides on the fix set of features x the number of input units will just be, the dimension of your features x(i) would be determined by that.</li>
<li>The number of output of this will be determined by the number of classes in your classification problem.</li>
<li>If you use more than one hidden layer, again the reasonable default will be to have the same number of hidden units in every single layer.</li>
<li>(<em>As for the number of hidden units - usually, the more hidden units the better</em>)</li>
</ul>
<p>What we need to implement in order to trade in neural network ?</p>
<ol>
<li>set up the neural network and to randomly initialize the values of the weights</li>
<li>forward propagation</li>
<li>compute this cost function [latex]J(\theta )[&#x2F;latex]</li>
<li>back-propagation</li>
<li>gradient checking</li>
<li>use an optimization algorithm</li>
</ol>
<h2 id="Autonomous-Driving"><a href="#Autonomous-Driving" class="headerlink" title="Autonomous Driving"></a>Autonomous Driving</h2><p>A fun and historically important example of Neural Network Learning, just so so.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2019/03/27/8-neural-networks-representation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Water">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Water's Home">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/03/27/8-neural-networks-representation/" class="post-title-link" itemprop="url">8 Neural Networks: Representation</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-03-27 14:07:58" itemprop="dateCreated datePublished" datetime="2019-03-27T14:07:58+08:00">2019-03-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-07-04 15:26:09" itemprop="dateModified" datetime="2022-07-04T15:26:09+08:00">2022-07-04</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/machine-learning/" itemprop="url" rel="index"><span itemprop="name">machine-learning</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/machine-learning/Machine-Learning-Offered-By-Stanford-University/" itemprop="url" rel="index"><span itemprop="name">Machine Learning Offered By Stanford University</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Non-linear-Hypotheses"><a href="#Non-linear-Hypotheses" class="headerlink" title="Non-linear Hypotheses"></a>Non-linear Hypotheses</h2><p>Neural Networks, which turns out to be a much better way to learn complex hypotheses, complex nonlinear hypotheses even when your input feature space, even when n is large.</p>
<h2 id="Neurons-and-the-Brain"><a href="#Neurons-and-the-Brain" class="headerlink" title="Neurons and the Brain"></a>Neurons and the Brain</h2><p>Neural Networks are a pretty old algorithm that was originally motivated by the goal of having machines that can mimic the brain. It’s actually a very effective state of the art technique for modern day machine learning applications.</p>
<h2 id="Model-Representation"><a href="#Model-Representation" class="headerlink" title="Model Representation"></a>Model Representation</h2><p>A neuron is is a computational unit that gets a number of inputs through its input wires, does some computation, and then it sends outputs, via its axon to other nodes or other neurons in the brain. <strong>Weights</strong> of a model just means exactly the same thing as parameters of the model. <strong>Forward Propagation</strong> : Because it start of with the activation of the input-units and then it sort of forward-propagate that to the hidden layer and then it sort of forward propagate that and compute the activation of the output layer. The more complex features will be better than x^n, and it will be more work well for prediction new data.</p>
<h2 id="Examples-and-Intuitions"><a href="#Examples-and-Intuitions" class="headerlink" title="Examples and Intuitions"></a>Examples and Intuitions</h2><p>In normal Logistic Regression, though we can use some polynomial to contract some features, we still be limited by original features.But in Neuron Network, the original features just be work on input layer. We can use contract neuron network to  be more complex neuron network that will do more complex compute.</p>
<h2 id="Multiclass-Classification"><a href="#Multiclass-Classification" class="headerlink" title="Multiclass Classification"></a>Multiclass Classification</h2><p>four output units represents four classification</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2019/03/27/7-regularization/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Water">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Water's Home">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/03/27/7-regularization/" class="post-title-link" itemprop="url">7 Regularization</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-03-27 11:56:33" itemprop="dateCreated datePublished" datetime="2019-03-27T11:56:33+08:00">2019-03-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-07-04 15:26:09" itemprop="dateModified" datetime="2022-07-04T15:26:09+08:00">2022-07-04</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/machine-learning/" itemprop="url" rel="index"><span itemprop="name">machine-learning</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/machine-learning/Machine-Learning-Offered-By-Stanford-University/" itemprop="url" rel="index"><span itemprop="name">Machine Learning Offered By Stanford University</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="The-Problem-of-Overfitting"><a href="#The-Problem-of-Overfitting" class="headerlink" title="The Problem of Overfitting"></a>The Problem of Overfitting</h2><p>Regularization, that will allow us to ameliorate or to reduce this overfitting problem and get these learning algorithms to <strong>maybe work much better</strong>. If you were to fit a very <strong>high-order polynomial</strong>, if you were to generate lots of high-order polynomial terms of speeches, then, logistical regression may contort itself, may try really hard to find a decision boundary that fits your training data or go to great lengths to contort itself, to fit every single training example well. But this really doesn’t look like a very good hypothesis, for <strong>making predictions</strong>. The term <strong>generalized</strong> refers to how well a hypothesis applies even to new examples. In order to address over fitting, there are two main options for things that we can do.*   reduce the number of features</p>
<ul>
<li>regularization, keep all the features, but we’re going to reduce the magnitude or the values of the parameters</li>
</ul>
<h2 id="Cost-Function"><a href="#Cost-Function" class="headerlink" title="Cost Function"></a>Cost Function</h2><h4 id="Orignal-Model"><a href="#Orignal-Model" class="headerlink" title="Orignal Model :"></a>Orignal Model :</h4><p>[latex] h_\theta(x) &#x3D; \theta_0 + \theta_1x_1 + \theta_2x\underset{2}{2} + \theta_3x\underset{3}{3} + \theta_4x\underset{4}{4} [&#x2F;latex]</p>
<h4 id="Modified-Model"><a href="#Modified-Model" class="headerlink" title="Modified Model :"></a>Modified Model :</h4><p>[latex] \underset{\theta }{min}\frac{1}{2m}[\sum_{i&#x3D;1}^{m}(h_\theta(x^{(i)}-y^{(i)})^2+1000\theta\underset{3}{2}+10000\theta \underset{4}{2})] [&#x2F;latex]</p>
<h4 id="Suppose"><a href="#Suppose" class="headerlink" title="Suppose :"></a>Suppose :</h4><p>[latex] J(\theta) &#x3D; \frac {1}{2m} [\sum_{i&#x3D;1}^{m} (h_\theta(x^{(i)}-y^{(i)})^2 + \lambda \sum_{j&#x3D;1}^{n} \theta _j^2] [&#x2F;latex]</p>
<p><strong>regularization parameter</strong> : [latex]\lambda[&#x2F;latex]</p>
<h4 id="Notice"><a href="#Notice" class="headerlink" title="Notice :"></a>Notice :</h4><p>[latex]\lambda \sum_{j&#x3D;1}^{n} \theta _j^2[&#x2F;latex]</p>
<p>The extra regularization term at the end to shrink every single parameter and so this term we tend to shrink all of my parameters.</p>
<h2 id="Regularized-Linear-Regression"><a href="#Regularized-Linear-Regression" class="headerlink" title="Regularized Linear Regression"></a>Regularized Linear Regression</h2><p>[latex] J(\theta) &#x3D; \frac {1}{2m} [\sum_{i&#x3D;1}^{m} (h_\theta(x^{(i)}-y^{(i)})^2 + \lambda \sum_{j&#x3D;1}^{n} \theta _j^2] [&#x2F;latex]</p>
<p>repeat until convergence {</p>
<p>[latex]\theta _0 :&#x3D; \theta _0 - \alpha \frac {1}{m} \sum _{i&#x3D;1}^{m} (h_\theta (x^{(i)}) - y^{(i)})x_0^{(i)}[&#x2F;latex] [latex]\theta _j :&#x3D; \theta _j - \alpha [\frac {1}{m} \sum _{i&#x3D;1}^{m} (h_\theta (x^{(i)}) - y^{(i)})x_j^{(i)} + \frac {\lambda }{m}\theta _j][&#x2F;latex]</p>
<p>}</p>
<h4 id="Modified"><a href="#Modified" class="headerlink" title="Modified :"></a>Modified :</h4><p>[latex]\theta _j :&#x3D; \theta _j(1 - \alpha \frac {\lambda }{m}) - \alpha \frac {1}{m} \sum _{i&#x3D;1}^{m} (h_\theta (x^{(i)}) - y^{(i)})x_j^{(i)}[&#x2F;latex]</p>
<h2 id="Regularized-Logistic-Regression"><a href="#Regularized-Logistic-Regression" class="headerlink" title="Regularized Logistic Regression"></a>Regularized Logistic Regression</h2><p>[latex]J(\theta) &#x3D; \frac {1}{m}\sum _{i&#x3D;1}^{m} [-y^{(i)} log(h_\theta (x^{(i)})) - (1 - y^{(i)})log(1 - h_\theta (x^{(i)}))] + \frac {\lambda}{2m} \sum _{i&#x3D;1}^{n} \theta _j^{2}[&#x2F;latex]</p>
<h4 id="Python-Code"><a href="#Python-Code" class="headerlink" title="Python Code :"></a>Python Code :</h4><p>import numpy as np<br>def costReg(theta, X, y, learningRate):<br>theta &#x3D; np.matrix(theta)<br>X &#x3D; np.matrix(X)<br>y &#x3D; np.matrix(y)<br>first &#x3D; np.multiply(-y, np.log(sigmoid(X * theta.T)))<br>second &#x3D; np.multiply((1 - y), np.log(1 - sigmoid(X * theta.T)))<br>reg &#x3D; learningRate &#x2F; (2 * len(X)) * np.sum(np.power(theta[:,1:theta.shape[1]], 2))<br>return np.sum(first - second) &#x2F; len(X) + reg</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2019/03/26/6-logistic-regression/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Water">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Water's Home">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/03/26/6-logistic-regression/" class="post-title-link" itemprop="url">6 Logistic Regression</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-03-26 14:40:42" itemprop="dateCreated datePublished" datetime="2019-03-26T14:40:42+08:00">2019-03-26</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-07-04 15:26:09" itemprop="dateModified" datetime="2022-07-04T15:26:09+08:00">2022-07-04</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/machine-learning/" itemprop="url" rel="index"><span itemprop="name">machine-learning</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/machine-learning/Machine-Learning-Offered-By-Stanford-University/" itemprop="url" rel="index"><span itemprop="name">Machine Learning Offered By Stanford University</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Classification"><a href="#Classification" class="headerlink" title="Classification"></a>Classification</h2><p>Classification : [latex]y &#x3D; [&#x2F;latex] 0 or 1 [latex]h_\theta (x)[&#x2F;latex] can be &gt; 1 or &lt; 0</p>
<p>Logistic Regression: [latex]0 \leq h_\theta (x) \leq 1 [&#x2F;latex] Logistic regression which has the property that the output, the predictions of logistic regression are always between zero and one. Logistic Regression is actually a classification algorithm.</p>
<h2 id="Hypothesis-Representation"><a href="#Hypothesis-Representation" class="headerlink" title="Hypothesis Representation"></a>Hypothesis Representation</h2><p><strong>Sigmoid function</strong> : [latex]g(z) &#x3D; \frac {1}{1+e^{-z}}[&#x2F;latex]</p>
<p>import numpy as np<br>def sigmoid(z):<br>    return 1 &#x2F; (1 + np.exp(-z))</p>
<h2 id="Decision-Boundary"><a href="#Decision-Boundary" class="headerlink" title="Decision Boundary"></a>Decision Boundary</h2><p>Much higher order polynomials, then it’s possible to show that you can get even more complex decision boundaries and logistic regression can be used to find the zero boundaries.</p>
<h2 id="Cost-Function"><a href="#Cost-Function" class="headerlink" title="Cost Function"></a>Cost Function</h2><p>How to fit the parameters theta for logistic regression. In particular, I’d like to define the optimization objective or the cost function that we’ll use to fit the parameters. Here’s to supervised learning problem of fitting a logistic regression model.</p>
<h4 id="Linear-regression-the-cost-function"><a href="#Linear-regression-the-cost-function" class="headerlink" title="Linear regression the cost function"></a>Linear regression the cost function</h4><p>[latex] J(\theta ) &#x3D; \frac {1}{m} \sum_{i &#x3D; 1}^{m} \frac {1}{2}(h_\theta(x^{(i)}) - y^{(i)})^{2} [&#x2F;latex]</p>
<h4 id="Logistic-regression-the-cost-function"><a href="#Logistic-regression-the-cost-function" class="headerlink" title="Logistic regression the cost function"></a>Logistic regression the cost function</h4><p>[latex] J(\theta ) &#x3D; \frac {1}{m} \sum_{i &#x3D; 1}^{m} Cost(h_\theta(x^{(i)}), y^{(i)}) [&#x2F;latex]</p>
<p>[latex] Cost(h_\theta (x), y) &#x3D; \begin{cases} -log(h_\theta(x)) &amp; \text{ if } y&#x3D;1 \\ -log(1 - h_\theta(x)) &amp; \text{ if } y&#x3D;0 \end{cases} [&#x2F;latex]</p>
<h2 id="Simplified-Cost-Function-and-Gradient-Descent"><a href="#Simplified-Cost-Function-and-Gradient-Descent" class="headerlink" title="Simplified Cost Function and Gradient Descent"></a>Simplified Cost Function and Gradient Descent</h2><p>How to implement a fully working version of logistic regression. It’s too hard to notes here , if you are interested in details, you can visit <a target="_blank" rel="noopener" href="https://www.coursera.org/learn/machine-learning">coursera.org</a> A vectorized implementation can update, you know, all of these N plus 1 parameters all in one fell swoop. Feature scaling can help gradient descents converge faster for linear regression. The idea of feature scaling also applies to gradient descent for logistic regression.</p>
<h2 id="Advanced-Optimization"><a href="#Advanced-Optimization" class="headerlink" title="Advanced Optimization"></a>Advanced Optimization</h2><p>For gradient descent, I guess technically you don’t actually need code to compute the cost function [latex]J_\theta[&#x2F;latex]. You only need code to compute the derivative terms. Conjugate gradient BFGS and L-BFGS are examples of more sophisticated optimization algorithms. <strong>These algorithms have a number of advantages:</strong>*   do not need to manually pick the learning rate alpha.</p>
<p><em>It is actually entirely possible to use these algorithms successfully and apply to lots of different learning problems without actually understanding the inter-loop of what these algorithms do.</em> For these algorithms also what I would recommend you do is just <strong>use a software library</strong>. Sophisticated optimization library, it makes the just a little bit more opaque and so just maybe a little bit harder to debug. But these algorithms <strong>often run much faster</strong> than gradient descent. If you have <strong>a large machine learning problem</strong>, you can use these algorithms instead of using gradient descent.</p>
<h2 id="Multiclass-Classification-One-vs-all"><a href="#Multiclass-Classification-One-vs-all" class="headerlink" title="Multiclass Classification_ One-vs-all"></a>Multiclass Classification_ One-vs-all</h2><p>one-versus-all classification Do the same thing for the third class and fit a third classifier H superscript 3 of X and maybe this or give us a classifier that separates the positive and negative examples like that. Basically pick the classifier, pick whichever one of the three classifiers is most confident, or most enthusistically says that it thinks it has a right class. And with this little method you can now take the logistic regression classifier and make it work on multi-class classification problems as well.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2019/03/26/5-octave-tutorial/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Water">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Water's Home">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/03/26/5-octave-tutorial/" class="post-title-link" itemprop="url">5 Octave Tutorial</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-03-26 14:35:24" itemprop="dateCreated datePublished" datetime="2019-03-26T14:35:24+08:00">2019-03-26</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-07-04 15:26:09" itemprop="dateModified" datetime="2022-07-04T15:26:09+08:00">2022-07-04</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/machine-learning/" itemprop="url" rel="index"><span itemprop="name">machine-learning</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/machine-learning/Machine-Learning-Offered-By-Stanford-University/" itemprop="url" rel="index"><span itemprop="name">Machine Learning Offered By Stanford University</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Basic-Operations"><a href="#Basic-Operations" class="headerlink" title="Basic Operations"></a>Basic Operations</h2><p>If you want to build a large scale deployment of a learning algorithm, what people will often do is prototype and the language is <strong>Octave</strong>. Get your learning algorithms to work more quickly in Octave. Then overall you have a huge time savings by <strong>first developing the algorithms in Octave</strong>, and then <strong>implementing and maybe C++ or Java</strong>, only after we have the ideas working. Octave is nice because open sourced.*   % : comment</p>
<ul>
<li>~&#x3D; : not equal</li>
<li>; : suppresses the print output</li>
<li>DISP : For more complex printing</li>
</ul>
<p>V&#x3D;1：0.1：2 % it sets V to the bunch of elements that start from 1. And increments and steps of 0.1 until you get up to 2.</p>
<p>ones(2, 3) % generates a matrix that is a two by three matrix that is the matrix of all ones.</p>
<p>C &#x3D; 2 * ones(2, 3) % that is all two’s.</p>
<p>w &#x3D; zeros(1, 3) % that is all zero’s.</p>
<p>rand(3,3)</p>
<p>w &#x3D; rand(1, 3) % normal random variable</p>
<p>hist % plot a histogram</p>
<p>help</p>
<h2 id="Moving-Data-Around"><a href="#Moving-Data-Around" class="headerlink" title="Moving Data Around"></a>Moving Data Around</h2><p>size(A) % the size of a matrix</p>
<p>size(A, 1) % the first dimension of A, size of the first dimension of A.</p>
<p>length(v) % the size of the longest dimension.</p>
<p>load(‘featureX.dat’)</p>
<p>who % the variables that Octave has in memory currently</p>
<p>whos % the detailed view</p>
<p>clear featuresX</p>
<p>save hello.mat v % save the variable V into a file called hello.mat.</p>
<p>save hello.txt v -ascii % a human readable format</p>
<p>A(3,2)</p>
<p>A(2,:) % fetch everything in the second row.</p>
<p>A([1 3],:) % get all of the elements of A who’s first indexes one or three.</p>
<p>A &#x3D; [A, [100, 101, 102]] % this will do is add another column vector to the right.</p>
<p>A(:) % put all elements with A into a single column vector</p>
<p>C &#x3D; [A B] % taking these two matrices and just concatenating onto each other.</p>
<p>C &#x3D; [A; B] % The semicolon notation means that I go put the next thing at the bottom.</p>
<p><strong>There’s no point at all to try to memorize all these commands.</strong> It’s just, but what you should do is, hopefully from this video you have gotten a sense of the sorts of things you can do.</p>
<h2 id="Computing-on-Data"><a href="#Computing-on-Data" class="headerlink" title="Computing on Data"></a>Computing on Data</h2><p>AxC % multiply 2 of matrices</p>
<p>A .* B % take each elements of A and multiply it by the corresponding elements of B.</p>
<p>A .^ 2 % the element wise squaring of A</p>
<p>1 .&#x2F; V % the element wise reciprocal of V</p>
<p>log(v) % an element wise logarithm of v</p>
<p>exp(v) </p>
<p>abs(V) % the element wise absolute value of V</p>
<p>-v % the same as -1 x V</p>
<p>v + ones(3,1) % this increments V by one.</p>
<p>v + 1 % another simpler way</p>
<p>A’ % the apostrophe symbol, a transpose of A</p>
<p>val&#x3D;max(a) % set val equals max of A</p>
<p>[val, ind] &#x3D; max(a) % val &#x3D; the maximum value, ind &#x3D; the index</p>
<p>a &lt; 3 % a &#x3D; [1 15 2 0.5], the result will be [1 0 1 1]</p>
<p>find(a &lt; 3) % [1 3 4]</p>
<p>A &#x3D; magic(3) % Returns this matrices called magic squares that all of their rows and columns and diagonals sum up to the same thing.</p>
<p>[r,c] &#x3D; find( A&gt;&#x3D;7 ) % This finds all the elements of a that are greater than and equals to 7 and so, R C sense a row and column.</p>
<p>sum(a) % This adds up all the elements of A.</p>
<p>prod(a) % multiply them</p>
<p>floor(a) % Floor A rounds down</p>
<p>ceil(A) % rounded up</p>
<p>type(3) % sets a 3 by 3 matrix</p>
<p>max(A,[],1) % takes the column wise maximum</p>
<p>max(A,[],2) % takes the per row maximum</p>
<p>max(A) % it defaults to column</p>
<p>max(max(A)) % the maximum element in the entire matrix A</p>
<p>sum(A,1) % does a per column sum</p>
<p>sum(A,2) % do the row wise sum</p>
<p>eye(9) % nine identity matrix</p>
<p>sum(sum(A.*eye(9)) % the sum of these diagonal elements</p>
<p>flipup&#x2F;flipud % Flip UD stands for flip up&#x2F;down</p>
<p>pinv(A) % a pseudo inference</p>
<p>After running a learning algorithm, often one of the most useful things is to be able to look at your results, or to plot, or visualize your result.</p>
<h2 id="Plotting-Data"><a href="#Plotting-Data" class="headerlink" title="Plotting Data"></a>Plotting Data</h2><p>Often, plots of the data or of all the learning algorithm outputs will also give you ideas for how to improve your learning algorithm.</p>
<p>t &#x3D; [0:0.01:0.98]<br>y1 &#x3D; sin(2 * pi * 4 * t)<br>plot(t, y1) % plot the sine function</p>
<p>y2 &#x3D; cos(2 * pi * 4 * 4)<br>plot(t, y2)</p>
<p>hold on % figures on top of the old one</p>
<p>plot(t, y2, ‘r’) % different color</p>
<p>xlabel(‘time’) % label the X axis, or the horizontal axis<br>ylabel(‘value’)</p>
<p>legend(‘sin’, ‘cos’) % puts this legend up on the upper right showing what the 2 lines are</p>
<p>title(‘myplot’) % the title at the top of this figure</p>
<p>print -dpng ‘myplot.png’ % save figure</p>
<p>close % disappeared</p>
<p>figure(1); plot(t, y1); % Starts up first figure, and that plots t, y1.<br>figure(2); plot(t, y2); </p>
<p>subplot(1,2,1) % sub-divides the plot into a one-by-two grid with the first 2 parameters are<br>plot(t, y1) % fills up this first element<br>subplot(1,2,2)<br>plot(t, y2)</p>
<p>axis([0.5 1 -1 1]) % sets the x range and y range for the figure on the right</p>
<p>clf % clear</p>
<p>imagesc(A) % visualize the matrix and the different colors correspond to the different values in the A matrix.<br>colormap gray % color map gray</p>
<p>imagesc(magic(15))，colorbar，colormap gray % running three commands at a time</p>
<h2 id="Control-Statements-for-while-if-statements"><a href="#Control-Statements-for-while-if-statements" class="headerlink" title="Control Statements_ for, while, if statements"></a>Control Statements_ for, while, if statements</h2><p>for i &#x3D; 1 : 10,<br>    v(i) &#x3D; 2 ^ i;<br>end;</p>
<p>indices &#x3D; 1 : 10;<br>for i &#x3D; indices,<br>    disp(i);<br>end;</p>
<p>i &#x3D; 1;<br>while i &lt;&#x3D; 5,<br>    v(i) &#x3D; 100;<br>    i &#x3D; i + 1;<br>end;</p>
<p>i &#x3D; 1;<br>while true,<br>    v(i) &#x3D; 999;<br>    i &#x3D; i + 1;<br>    if i &#x3D;&#x3D; 6,<br>        break;<br>    end;<br>end;</p>
<p>v(1) &#x3D; 2;<br>if v(1) &#x3D;&#x3D; 1,<br>    disp(‘The value is one’);<br>elseif v(1) &#x3D;&#x3D; 2,<br>    disp(‘The value is two’);<br>else,<br>    disp(‘The value is not one or two’);<br>end;</p>
<p>function name (arg-list)<br>  body<br>endfunction</p>
<p>function wakeup (message)<br>  printf (“\a%s\n”, message);<br>endfunction</p>
<p>wakeup (“Rise and shine!”);</p>
<p>function y &#x3D; squareThisNumber(x)</p>
<p>addpath % add path for search dirs</p>
<p>[a, b] &#x3D; SquareAndCubeThisNumber(5) % a &#x3D; 25, b &#x3D; 125</p>
<h2 id="Vectorization"><a href="#Vectorization" class="headerlink" title="Vectorization"></a>Vectorization</h2><h4 id="Unvectorized-implementation"><a href="#Unvectorized-implementation" class="headerlink" title="Unvectorized implementation"></a>Unvectorized implementation</h4><p>[latex]h_\theta (x) &#x3D; \sum _{j&#x3D;0}^{n} \theta _jx_j[&#x2F;latex]</p>
<p>prediction &#x3D; 0.0;<br>for j &#x3D; 1 : n + 1,<br>    prediction &#x3D; prediction + theta(j) * x(j)<br>end;</p>
<h4 id="Vectorized-implementation"><a href="#Vectorized-implementation" class="headerlink" title="Vectorized implementation"></a>Vectorized implementation</h4><p>[latex]h_\theta (x) &#x3D; \theta ^Tx[&#x2F;latex]</p>
<p>prediction &#x3D; theta’ * x;</p>
<p>Using a vectorized implementation, you should be able to get a much more efficient implementation of linear regression.</p>
<h2 id="Working-on-and-Submitting-Programming-Exercises"><a href="#Working-on-and-Submitting-Programming-Exercises" class="headerlink" title="Working on and Submitting Programming Exercises"></a>Working on and Submitting Programming Exercises</h2><p>How to use the submission system which will let you verify right away that you got the right answer for your machine learning program exercise. If you are interested in details, you can visit <a target="_blank" rel="noopener" href="https://www.coursera.org/learn/machine-learning">coursera.org</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2019/03/25/4-linear-regression-with-multiple-variables/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Water">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Water's Home">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/03/25/4-linear-regression-with-multiple-variables/" class="post-title-link" itemprop="url">4 Linear Regression with Multiple Variables</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-03-25 16:57:28" itemprop="dateCreated datePublished" datetime="2019-03-25T16:57:28+08:00">2019-03-25</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-07-04 15:26:09" itemprop="dateModified" datetime="2022-07-04T15:26:09+08:00">2022-07-04</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/machine-learning/" itemprop="url" rel="index"><span itemprop="name">machine-learning</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/machine-learning/Machine-Learning-Offered-By-Stanford-University/" itemprop="url" rel="index"><span itemprop="name">Machine Learning Offered By Stanford University</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Linear-Regression-with-Multiple-Variables"><a href="#Linear-Regression-with-Multiple-Variables" class="headerlink" title="Linear Regression with Multiple Variables"></a>Linear Regression with Multiple Variables</h2><p>[latex]x^{(i)}_j[&#x2F;latex] : refer to feature number <strong>j</strong> in the <strong>x</strong> factor.</p>
<h2 id="Gradient-Descent-for-Multiple-Variables"><a href="#Gradient-Descent-for-Multiple-Variables" class="headerlink" title="Gradient Descent for Multiple Variables"></a>Gradient Descent for Multiple Variables</h2><p>def computeCost(X, y, theta):<br>    inner &#x3D; np.power(((X * theta.T) - y), 2)<br>    return np.sum(inner) &#x2F; (2 * len(X))</p>
<h2 id="Gradient-Descent-in-Practice-I-Feature-Scaling"><a href="#Gradient-Descent-in-Practice-I-Feature-Scaling" class="headerlink" title="Gradient Descent in Practice I - Feature Scaling"></a>Gradient Descent in Practice I - Feature Scaling</h2><p>the different features take on similar ranges of values, then gradient descents can converge more quickly. [latex] x_n &#x3D; \frac {x_n -\mu _n}{s_n} [&#x2F;latex] [latex]\mu _n[&#x2F;latex] : the average value [latex]s_n[&#x2F;latex] : the range of values</p>
<h2 id="Gradient-Descent-in-Practice-II-Learning-Rate"><a href="#Gradient-Descent-in-Practice-II-Learning-Rate" class="headerlink" title="Gradient Descent in Practice II - Learning Rate"></a>Gradient Descent in Practice II - Learning Rate</h2><p>Looking at figure that pluck the cost function j of theta as gradient descent runs and the x-axis here is the number of iteration of gradient descent and as gradient descent runs. Maybe [latex]\alpha &#x3D; 0.01, 0.03, 0.1, 0.3, 1, 3, 10[&#x2F;latex]</p>
<h2 id="Features-and-Polynomial-Regression"><a href="#Features-and-Polynomial-Regression" class="headerlink" title="Features and Polynomial Regression"></a>Features and Polynomial Regression</h2><p>Look at the data and choose features. You can put polynomial functions as well and sometimes by appropriate insight into the feature simply get a much better model for your data. Feature Scaling is very necessary if you use polynomial functions.</p>
<h2 id="Normal-Equation"><a href="#Normal-Equation" class="headerlink" title="Normal Equation"></a>Normal Equation</h2><p>The normal equation, which for some linear regression problems, will give us a much better way to solve for the optimal value of the parameters theta. [latex]\theta &#x3D; (X^{T}X)^{-1}X^{T}y[&#x2F;latex]</p>
<h4 id="Disadvantages-of-gradient-descent"><a href="#Disadvantages-of-gradient-descent" class="headerlink" title="Disadvantages of gradient descent"></a>Disadvantages of gradient descent</h4><ul>
<li>choose the learning rate Alpha.</li>
<li>many more iterations</li>
</ul>
<h4 id="Advantages-of-gradient-descent"><a href="#Advantages-of-gradient-descent" class="headerlink" title="Advantages of gradient descent"></a>Advantages of gradient descent</h4><ul>
<li>works pretty well even if you have millions of features</li>
<li>many kinds of models</li>
</ul>
<h2 id="Normal-Equation-Noninvertibility"><a href="#Normal-Equation-Noninvertibility" class="headerlink" title="Normal Equation Noninvertibility"></a>Normal Equation Noninvertibility</h2><p>Some matrices are invertible and some matrices do not have an inverse we call those <strong>non-invertible matrices</strong>. Look at your features and see if you have redundant features or being a linear function of each other, and if you do have redundant features and if you just delete one of these features you really don’t need both of these features that will solve your <strong>non-invertibility problem</strong>.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/12/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/12/">12</a><span class="page-number current">13</span><a class="page-number" href="/page/14/">14</a><span class="space">&hellip;</span><a class="page-number" href="/page/24/">24</a><a class="extend next" rel="next" href="/page/14/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Water"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Water</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">233</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">65</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/linmonsv" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;linmonsv" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:qin2@qq.com" title="E-Mail → mailto:qin2@qq.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2017 – 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Water</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
