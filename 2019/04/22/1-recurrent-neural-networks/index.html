<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"always","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="Why Sequence Models?Models like recurrent neural networks or RNNs have transformed speech recognition, natural language processing and other areas.  NotationSuppose the input is the sequence of nine w">
<meta property="og:type" content="article">
<meta property="og:title" content="1 Recurrent Neural Networks">
<meta property="og:url" content="http://example.com/2019/04/22/1-recurrent-neural-networks/index.html">
<meta property="og:site_name" content="Water&#39;s Home">
<meta property="og:description" content="Why Sequence Models?Models like recurrent neural networks or RNNs have transformed speech recognition, natural language processing and other areas.  NotationSuppose the input is the sequence of nine w">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/img/Examples_of_sequence_data.png">
<meta property="og:image" content="http://example.com/img/Notation_Representing_words.png">
<meta property="og:image" content="http://example.com/img/RNN_Forward_Propagation.png">
<meta property="og:image" content="http://example.com/img/RNN_Back_Propagation.png">
<meta property="og:image" content="http://example.com/img/Summary_of_RNN_types.png">
<meta property="og:image" content="http://example.com/img/RNN_unit.png">
<meta property="og:image" content="http://example.com/img/LSTM_in_pictures.png">
<meta property="article:published_time" content="2019-04-22T08:55:03.000Z">
<meta property="article:modified_time" content="2022-07-05T01:29:38.996Z">
<meta property="article:author" content="Water">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/Examples_of_sequence_data.png">

<link rel="canonical" href="http://example.com/2019/04/22/1-recurrent-neural-networks/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>1 Recurrent Neural Networks | Water's Home</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Water's Home</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Just another Life Style</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2019/04/22/1-recurrent-neural-networks/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Water">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Water's Home">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          1 Recurrent Neural Networks
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-04-22 16:55:03" itemprop="dateCreated datePublished" datetime="2019-04-22T16:55:03+08:00">2019-04-22</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-07-05 09:29:38" itemprop="dateModified" datetime="2022-07-05T09:29:38+08:00">2022-07-05</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/machine-learning/" itemprop="url" rel="index"><span itemprop="name">machine-learning</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/machine-learning/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/deep-learning/" itemprop="url" rel="index"><span itemprop="name">deep-learning</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/deep-learning/Deep-Learning-Specialization-Offered-By-deeplearning-ai/" itemprop="url" rel="index"><span itemprop="name">Deep Learning Specialization Offered By deeplearning.ai</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="Why-Sequence-Models"><a href="#Why-Sequence-Models" class="headerlink" title="Why Sequence Models?"></a>Why Sequence Models?</h2><p>Models like <strong>recurrent neural networks</strong> or <strong>RNNs</strong> have transformed speech recognition, natural language processing and other areas. <img src="/img/Examples_of_sequence_data.png"></p>
<h2 id="Notation"><a href="#Notation" class="headerlink" title="Notation"></a>Notation</h2><p>Suppose the input is the sequence of nine words. So, eventually we’re going to have nine sets of features to represent these nine words, and index into the positions in the sequence, I’m going to use [latex]x^{&lt;1&gt;}[&#x2F;latex], [latex]x^{&lt;2&gt;}[&#x2F;latex], [latex]x^{&lt;3&gt;}[&#x2F;latex] and so on up to [latex]x^{&lt;9&gt;}[&#x2F;latex] to index into the different positions. use [latex]x^{<t>}[&#x2F;latex] to index into positions, in the middle of the sequence. And t implies that these are temporal sequences although whether the sequences are temporal one or not, I’m going to use the index t to index into the positions in the sequence. Used [latex]T_{x}[&#x2F;latex] denote the length of the input sequence, [latex]x^{(i)<t>}[&#x2F;latex] refer to the Tth element or the Tth element in the sequence of training example i [latex]T_{x}^{(i)}[&#x2F;latex] is the length of sequence i <strong>NLP</strong> or Natural Language Processing Use <strong>one-hot representations</strong> to represent each of these words. <img src="/img/Notation_Representing_words.png"> What if you encounter a word that is not in your vocabulary? Well the answer is, you create a new token or a new fake word called Unknown Word which under note as follows angle brackets <strong>UNK</strong> to represent words not in your vocabulary.</p>
<h2 id="Recurrent-Neural-Network-Model"><a href="#Recurrent-Neural-Network-Model" class="headerlink" title="Recurrent Neural Network Model"></a>Recurrent Neural Network Model</h2><p><strong>Why not a standard network?</strong> <strong>Problems:</strong></p>
<ul>
<li>Inputs, outputs can be different lengths in different examples.</li>
<li>Doesn’t share features learned across different positions of text.</li>
</ul>
<p>And <strong>what a recurrent neural network does is</strong> when it then goes on to read the second word in a sentence, say X2, instead of just predicting Y2 using only X2, it also gets to input some information from what had computed that time-step one’s. <strong>At each time-step,  the recurrent neural network passes on this activation to the next time-step for it to use</strong>. Now <strong>one limitation of this</strong> particular neural network structure is that the prediction at a certain time <strong>uses</strong> inputs or uses information from the inputs <strong>earlier in the sequence but not information later</strong> in the sequence. We will address this in a later video where we talk about a <strong>bidirectional recurrent neural networks</strong> or <strong>BRNNs</strong>. The activation function used in to compute the activations will often be a tanh and the choice of an RNN and sometimes, Relu are also used although the tanh is actually a pretty common choice. Simplified RNN notation : [latex]\begin{matrix} a^{<t>} &#x3D; g_1(W_{aa}a^{<t-1>} + W_{ax}x^{<t>} + b_a)\\ \hat y ^{<t>} &#x3D; g_2(W_{ya}a^{<t>} + b_y) \end{matrix}[&#x2F;latex] <img src="/img/RNN_Forward_Propagation.png"></p>
<h2 id="Backpropagation-through-time"><a href="#Backpropagation-through-time" class="headerlink" title="Backpropagation through time"></a>Backpropagation through time</h2><p>As usual, when you implement this in one of the programming frameworks, <strong>often, the programming framework will automatically take care of backpropagation</strong>. <strong>Element-wise</strong> loss funtion : [latex]L^{<t>}(\hat y ^{<t>}, y ^{<t>}) &#x3D; -y ^{<t>}log \hat y ^{<t>} - (1 - \hat y ^{<t>})log(1 - \hat y ^{<t>})[&#x2F;latex] standard logistic regression loss also called the <strong>cross entropy loss</strong>. <strong>Overall</strong> loss of the entire sequence : [latex]L(\hat y, y) &#x3D; \sum _{t&#x3D;1}^{T_x} L ^{<t>}(\hat y ^{<t>}, y^{<t>})[&#x2F;latex] <strong>Backpropagation through time</strong>, And the motivation for this name is that for forward prop you are scanning from left to right, increasing indices of the time t, whereas the backpropagation, you’re going from right to left, kind of going backwards in time. <img src="/img/RNN_Back_Propagation.png"></p>
<h2 id="Different-types-of-RNNs"><a href="#Different-types-of-RNNs" class="headerlink" title="Different types of RNNs"></a>Different types of RNNs</h2><p><img src="/img/Summary_of_RNN_types.png"></p>
<h2 id="Language-model-and-sequence-generation"><a href="#Language-model-and-sequence-generation" class="headerlink" title="Language model and sequence generation"></a>Language model and sequence generation</h2><p><strong>What a language model does is</strong> given any sentence its job is to tell you <strong>what is the probability of a sentence</strong>, of that particular sentence. And this is a fundamental component for both speech recognition systems as you’ve just seen, as well as for machine translation systems where translation systems wants output. <strong>How do you build a language model?</strong></p>
<ul>
<li>first need a training set comprising a large corpus of English text. Or text from whatever language you want to build a language model of. And the word corpus is an NLP terminology that just means a large body or a very large set of English text of English sentences.<ul>
<li>The first thing you would do is tokenize this sentence. And that means you would form a vocabulary as we saw in an earlier video. And then map each of these words to, say, one-hot vectors, all to indices in your vocabulary.</li>
<li>One thing you might also want to do is model when sentences end. So another common thing to do is to add an extra token called a EOS.</li>
</ul>
</li>
<li>Go on to built the RNN model<ul>
<li>what [latex]a^{&lt;1&gt;}[&#x2F;latex] does is it will make a softmax prediction to try to figure out what is the probability of the first words y. And so that’s going to be y&lt;1&gt;. So what this step does is really, it has a softmax it’s trying to predict. What is the probability of any word in the dictionary?</li>
<li>Then, the RNN steps forward to the next step and has some activation, [latex]a^{&lt;1&gt;}[&#x2F;latex] to the next step. And at this step, this job is try to figure out, what is the second word?</li>
<li>whatever this given, everything that comes before, and hopefully it will predict that there’s a high chance of it, EOS end sentence token.</li>
</ul>
</li>
</ul>
<h2 id="Sampling-novel-sequences"><a href="#Sampling-novel-sequences" class="headerlink" title="Sampling novel sequences"></a>Sampling novel sequences</h2><p>After you train a sequence model, one of the ways you can informally get a sense of what is learned is to have a sample novel sequences.</p>
<ul>
<li>what you want to do is first sample what is the first word you want your model to generate.</li>
<li>…</li>
</ul>
<p>Then you will <strong>generate a randomly chosen sentence</strong> from your RNN language model.</p>
<ul>
<li><strong>words level RNN</strong></li>
<li><strong>character level RNN</strong><ul>
<li>advantage : you don’t ever have to worry about unknown word tokens.</li>
<li>disadvantage : you end up with much more, much longer sequences.<ul>
<li>so they are not in widespread used today. Except for maybe specialized applications where you might need to deal with unknown words or other vocabulary words a lot.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="Vanishing-gradients-with-RNNs"><a href="#Vanishing-gradients-with-RNNs" class="headerlink" title="Vanishing gradients with RNNs"></a>Vanishing gradients with RNNs</h2><p>It turns out the basics RNN we’ve seen so far it’s <strong>not very good at capturing very long-term dependencies</strong>.</p>
<ul>
<li>It turns out that <strong>vanishing gradients tends to be the bigger problem with training RNNs</strong>, </li>
<li>although when <strong>exploding gradients</strong> happens, it can be catastrophic because the exponentially large gradients can cause your parameters to become so large that your neural network parameters get really messed up. So it turns out that exploding gradients are easier to spot because the parameters just blow up and you might often see NaNs, or not a numbers, meaning results of a numerical overflow in your neural network computation. <ul>
<li>And if you do see exploding gradients, one solution to that is apply gradient clipping. And what that really means, all that means is look at your gradient vectors, and if it is bigger than some threshold, re-scale some of your gradient vector so that is not too big. So there are clips according to some maximum value. So if you see exploding gradients, if your derivatives do explode or you see NaNs, just apply <strong>gradient clipping</strong>, and that’s a relatively robust solution that will take care of exploding gradients.</li>
</ul>
</li>
</ul>
<p> </p>
<h2 id="Gated-Recurrent-Unit（GRU）"><a href="#Gated-Recurrent-Unit（GRU）" class="headerlink" title="Gated Recurrent Unit（GRU）"></a>Gated Recurrent Unit（GRU）</h2><p>The <strong>Gated Recurrent Unit</strong> which is a modification to the RNN hidden layer that makes it much better capturing long range connections and helps a lot with the vanishing gradient problems. <img src="/img/RNN_unit.png">                   The GRU unit is going to have a new variable called c which stands for cell, for <strong>memory cell</strong>. And what the memory cell do is it will provide a bit of memory to remember. [latex]\tilde{c} ^{<t>} &#x3D; tanh (W_c [c ^{<t-1>}, x ^{<t>}] + b_c)[&#x2F;latex] <strong>the important idea of the GRU :</strong> [latex]\begin{matrix} \Gamma _u &#x3D; \sigma(W_u[c^{<t-1>}, x^{<t>}] + b_u) \\ c^{<t>} &#x3D; \Gamma _u * \tilde{c} ^{<t>} + (1 - \Gamma _u) * c^{<t-1>} \end{matrix}[&#x2F;latex]</p>
<h2 id="LSTM（long-short-term-memory）unit"><a href="#LSTM（long-short-term-memory）unit" class="headerlink" title="LSTM（long short term memory）unit"></a>LSTM（long short term memory）unit</h2><p>the long short term memory units, and this is even more powerful than the GRU. <img src="/img/LSTM_in_pictures.png">   Perhaps, the most common one is that instead of just having the gate values be dependent only on a^{<t-1>} , x^{<t>}, sometimes, people also sneak in there the values c^{<t-1>} as well. This is called a <strong>peephole connection.</strong></p>
<h4 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h4><ul>
<li>relatively recent invention</li>
<li>a simpler model and so it is actually easier to build a much bigger network, it only has two gates, so computationally, it runs a bit faster. So, it scales the building somewhat bigger models</li>
</ul>
<h4 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h4><ul>
<li>actually came much earlier</li>
<li>more powerful and more flexible since it has three gates instead of two.</li>
</ul>
<p><strong>LSTM has been the historically more proven choice.</strong></p>
<h2 id="Bidirectional-RNN"><a href="#Bidirectional-RNN" class="headerlink" title="Bidirectional RNN"></a>Bidirectional RNN</h2><p><strong>Bidirectional RNNs</strong>, which lets you at a point in time to take information from both earlier and later in the sequence. In fact, for a lots of NLP problems, for a lot of text with natural language processing problems, <strong>a bidirectional RNN with a LSTM</strong> appears to <strong>be commonly used</strong>. The <strong>disadvantage</strong> of the bidirectional RNN is that you <strong>do need the entire sequence of data</strong> before you can make predictions anywhere.</p>
<h2 id="Deep-RNNs"><a href="#Deep-RNNs" class="headerlink" title="Deep RNNs"></a>Deep RNNs</h2><p>The different versions of RNNs you’ve seen so far will already work quite well by themselves. But for learning very complex functions sometimes <strong>it’s useful to stack multiple layers of RNNs together to build even deeper versions</strong> of these models. For RNNs, having three layers is already quite a lot. Because of the temporal dimension, these networks can already get quite big even if you have just a small handful of layers. And you don’t usually see these stacked up to be like 100 layers. One thing you do see sometimes is that you have recurrent layers that are stacked on top of each other. But then you might take the output here, let’s get rid of this, and then <strong>just have a bunch of deep layers that are not connected horizontally but have a deep network here</strong> that then finally predicts y&lt;1&gt;.</p>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2019/04/22/4-special-applications-face-recognition-neural-style-transfer/" rel="prev" title="4 Special applications : Face recognition & Neural style transfer">
      <i class="fa fa-chevron-left"></i> 4 Special applications : Face recognition & Neural style transfer
    </a></div>
      <div class="post-nav-item">
    <a href="/2019/04/23/2-natural-language-processing-and-word-embeddings/" rel="next" title="2 Natural Language Processing and Word Embeddings">
      2 Natural Language Processing and Word Embeddings <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Why-Sequence-Models"><span class="nav-number">1.</span> <span class="nav-text">Why Sequence Models?</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Notation"><span class="nav-number">2.</span> <span class="nav-text">Notation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Recurrent-Neural-Network-Model"><span class="nav-number">3.</span> <span class="nav-text">Recurrent Neural Network Model</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Backpropagation-through-time"><span class="nav-number">4.</span> <span class="nav-text">Backpropagation through time</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Different-types-of-RNNs"><span class="nav-number">5.</span> <span class="nav-text">Different types of RNNs</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Language-model-and-sequence-generation"><span class="nav-number">6.</span> <span class="nav-text">Language model and sequence generation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Sampling-novel-sequences"><span class="nav-number">7.</span> <span class="nav-text">Sampling novel sequences</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Vanishing-gradients-with-RNNs"><span class="nav-number">8.</span> <span class="nav-text">Vanishing gradients with RNNs</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Gated-Recurrent-Unit%EF%BC%88GRU%EF%BC%89"><span class="nav-number">9.</span> <span class="nav-text">Gated Recurrent Unit（GRU）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LSTM%EF%BC%88long-short-term-memory%EF%BC%89unit"><span class="nav-number">10.</span> <span class="nav-text">LSTM（long short term memory）unit</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#GRU"><span class="nav-number">10.0.1.</span> <span class="nav-text">GRU</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#LSTM"><span class="nav-number">10.0.2.</span> <span class="nav-text">LSTM</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Bidirectional-RNN"><span class="nav-number">11.</span> <span class="nav-text">Bidirectional RNN</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Deep-RNNs"><span class="nav-number">12.</span> <span class="nav-text">Deep RNNs</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Water"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Water</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">233</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">65</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/linmonsv" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;linmonsv" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:qin2@qq.com" title="E-Mail → mailto:qin2@qq.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2017 – 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Water</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
