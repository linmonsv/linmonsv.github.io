<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="Neural Network OverviewRefer to Example : [latex]x^{(i)}[&#x2F;latex] Refer to Layer : [latex]\alpha^{[m]}[&#x2F;latex] Algorithm 3.1 : [latex]\left.\begin{matrix} x\\ w\\ b \end{matrix}\right\} \Righ">
<meta property="og:type" content="article">
<meta property="og:title" content="3 Shallow Neural Networks">
<meta property="og:url" content="http://example.com/2019/04/16/3-shallow-neural-networks/index.html">
<meta property="og:site_name" content="Water&#39;s Home">
<meta property="og:description" content="Neural Network OverviewRefer to Example : [latex]x^{(i)}[&#x2F;latex] Refer to Layer : [latex]\alpha^{[m]}[&#x2F;latex] Algorithm 3.1 : [latex]\left.\begin{matrix} x\\ w\\ b \end{matrix}\right\} \Righ">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2019-04-16T08:40:13.000Z">
<meta property="article:modified_time" content="2022-07-04T07:26:09.535Z">
<meta property="article:author" content="Water">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/2019/04/16/3-shallow-neural-networks/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>3 Shallow Neural Networks | Water's Home</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Water's Home</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Just another Life Style</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2019/04/16/3-shallow-neural-networks/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Water">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Water's Home">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          3 Shallow Neural Networks
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-04-16 16:40:13" itemprop="dateCreated datePublished" datetime="2019-04-16T16:40:13+08:00">2019-04-16</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-07-04 15:26:09" itemprop="dateModified" datetime="2022-07-04T15:26:09+08:00">2022-07-04</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/machine-learning/" itemprop="url" rel="index"><span itemprop="name">machine-learning</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/machine-learning/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/deep-learning/" itemprop="url" rel="index"><span itemprop="name">deep-learning</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/deep-learning/Deep-Learning-Specialization-Offered-By-deeplearning-ai/" itemprop="url" rel="index"><span itemprop="name">Deep Learning Specialization Offered By deeplearning.ai</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="Neural-Network-Overview"><a href="#Neural-Network-Overview" class="headerlink" title="Neural Network Overview"></a>Neural Network Overview</h2><p>Refer to Example : [latex]x^{(i)}[&#x2F;latex] Refer to Layer : [latex]\alpha^{[m]}[&#x2F;latex] Algorithm 3.1 : [latex]\left.\begin{matrix} x\\ w\\ b \end{matrix}\right\} \Rightarrow z &#x3D; w^Tx + b[&#x2F;latex] Algorithm 3.2 : [latex]\left.\begin{matrix} x\\ w\\ b \end{matrix}\right\} \Rightarrow z &#x3D; w^Tx + b \Rightarrow \alpha &#x3D; \sigma(z) \Rightarrow L(a,y) \ (Loss \ Function)[&#x2F;latex] Algorithm 3.3 : [latex]\left.\begin{matrix} x\\ W^{[1]}\\ b^{[1]} \end{matrix}\right\} \Rightarrow z^{[1]} &#x3D; W^{[1]}x + b^{[1]} \Rightarrow \alpha^{[1]} &#x3D; \sigma(z^{[1]})[&#x2F;latex] Algorithm 3.4 : [latex]\left.\begin{matrix} x\\ dW^{[1]}\\ db^{[1]} \end{matrix}\right\} \Leftarrow dz^{[1]} &#x3D; d(W^{[1]}x + b^{[1]}) \Leftarrow d\alpha^{[1]} &#x3D; d\sigma(z^{[1]})[&#x2F;latex] Algorithm 3.5 : [latex]\left.\begin{matrix} x\\ dW^{[1]}\\ db^{[1]} \end{matrix}\right\} \Leftarrow dz^{[1]} &#x3D; d(W^{[1]}x + b^{[1]}) \Leftarrow d\alpha^{[1]} &#x3D; d\sigma(z^{[1]})[&#x2F;latex] Algorithm 3.6 : [latex]\left.\begin{matrix} d\alpha^{[1]} &#x3D; d\sigma(z^{[1]})\\ dW^{[2]}\\ db^{[2]} \end{matrix}\right\} \Leftarrow dz^{[2]} &#x3D; d(W^{[2]}\alpha^{[1]} + b^{[2]}) \Leftarrow d\alpha^{[2]} &#x3D; d\sigma(z^{[2]}) \Leftarrow dL(a^{[2]}, y)[&#x2F;latex]</p>
<h2 id="Neural-Network-Representation"><a href="#Neural-Network-Representation" class="headerlink" title="Neural Network Representation"></a>Neural Network Representation</h2><ul>
<li>input layer</li>
<li>hidden layer</li>
<li>output layer</li>
</ul>
<p>[latex]a[&#x2F;latex] : activations [latex]a^{[0]}[&#x2F;latex] : the activations of the input layer [latex]a^{[0]}_1[&#x2F;latex] : first node Algorithm 3.7 : [latex]a^{[1]}&#x3D;\begin{bmatrix} a^{[1]}_1\\ a^{[1]}_2\\ a^{[1]}_3\\ a^{[1]}_4 \end{bmatrix}[&#x2F;latex]</p>
<ul>
<li>when we count layers in neural networks we don’t count the input layer so the hidden layer is layer 1</li>
<li>In our notational convention we’re calling the input layer layer 0</li>
</ul>
<p><strong>so a two layer neural network looks like a neural network with one hidden layer.</strong></p>
<h2 id="Computing-a-Neural-Network’s-output"><a href="#Computing-a-Neural-Network’s-output" class="headerlink" title="Computing a Neural Network’s output"></a>Computing a Neural Network’s output</h2><p>Symbols in neural networks：</p>
<ul>
<li>𝑥 : features</li>
<li>𝑎 : output</li>
<li>𝑊 : weight</li>
<li>superscript : layers</li>
<li>subscript : number of the items</li>
</ul>
<p>How this neural network computers outputs :</p>
<ol>
<li>[latex]z_1^{[1]} &#x3D; w_1^{[1]T}x + b_1^{[1]}[&#x2F;latex]</li>
<li>[latex]a_1^{[1]} &#x3D; \sigma(z_1^{[1]})[&#x2F;latex]</li>
<li>[latex]a_2^{[1]}, a_3^{[1]}, a_4^{[1]}[&#x2F;latex]</li>
</ol>
<p>Vectorizing : stack nodes in a layer vertically Algorithm 3.10 : [latex]a^{[1]} &#x3D; \begin{bmatrix} a_1^{[1]}\\ a_2^{[1]}\\ a_3^{[1]}\\ a_4^{[1]} \end{bmatrix} &#x3D; \sigma(z^{[1]})[&#x2F;latex] Algorithm 3.11 : [latex]\begin{bmatrix} z_1^{[1]}\\ z_2^{[1]}\\ z_3^{[1]}\\ z_4^{[1]} \end{bmatrix} &#x3D; \begin{bmatrix} \cdots W_1^{[1]T} \cdots \\ \cdots W_2^{[1]T} \cdots \\ \cdots W_3^{[1]T} \cdots \\ \cdots W_4^{[1]T} \cdots \end{bmatrix} * \begin{bmatrix} x_1\\ x_2\\ x_3 \end{bmatrix} + \begin{bmatrix} b_1^{[1]}\\ b_2^{[1]}\\ b_3^{[1]}\\ b_4^{[1]} \end{bmatrix}[&#x2F;latex]</p>
<h2 id="Vectorizing-across-multiple-examples"><a href="#Vectorizing-across-multiple-examples" class="headerlink" title="Vectorizing across multiple examples"></a>Vectorizing across multiple examples</h2><p>Take the equations you had from the previous algorithm and with very little modification, change them to make the neural network compute the outputs on all the examples, pretty much all at the same time. [latex]a^{[2](i)}[&#x2F;latex] : Refers to training example i and layer two Algorithm 3.12 : [latex]x &#x3D; \begin{bmatrix} \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\ x^{(1)} &amp; x^{(2)} &amp; \cdots &amp; x^{(m)}\\ \vdots &amp; \vdots &amp; \vdots &amp; \vdots \end{bmatrix}[&#x2F;latex] Algorithm 3.13 : [latex]Z^{[1]} &#x3D; \begin{bmatrix} \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\ z^{[1](1)} &amp; z^{[1](2)} &amp; \cdots &amp; z^{[1](m)}\\ \vdots &amp; \vdots &amp; \vdots &amp; \vdots \end{bmatrix}[&#x2F;latex] Algorithm 3.14 : [latex]A^{[1]} &#x3D; \begin{bmatrix} \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\ a^{[1](1)} &amp; a^{[1](2)} &amp; \cdots &amp; a^{[1](m)}\\ \vdots &amp; \vdots &amp; \vdots &amp; \vdots \end{bmatrix}[&#x2F;latex] Algorithm 3.15 : [latex]\left.\begin{matrix} z^{[1](i)} &#x3D; W^{[1](i)}x^{(i)} + b^{[1]}\\ a^{[1](i)} &#x3D; \sigma(z^{[1](i)})\\ z^{[2](i)} &#x3D; W^{[2](i)}a^{[1](i)} + b^{[2]}\\ a^{[2](i)} &#x3D; \sigma(z^{[2](i)}) \end{matrix}\right\} \Rightarrow \left\{\begin{matrix} A^{[1]} &#x3D; \sigma (z^{[1]})\\ z^{[2]} &#x3D; W^{[2]}A^{[1]} + b^{[2]}\\ A^{[2]} &#x3D; \sigma (z^{[2]}) \end{matrix}\right.[&#x2F;latex]</p>
<h2 id="Justification-for-vectorized-implementation"><a href="#Justification-for-vectorized-implementation" class="headerlink" title="Justification for vectorized implementation"></a>Justification for vectorized implementation</h2><p>Algorithm 3.16 : [latex]\begin{matrix} z^{[1](1)} &#x3D; W^{[1]}x^{(1)} + b^{[1]}\\ z^{[1](2)} &#x3D; W^{[1]}x^{(2)} + b^{[1]}\\ z^{[1](3)} &#x3D; W^{[1]}x^{(3)} + b^{[1]} \end{matrix}[&#x2F;latex] Algorithm 3.17 : [latex]W^{[1]}x &#x3D; \begin{bmatrix} \cdots \\ \cdots \\ \cdots \end{bmatrix} \begin{bmatrix} \vdots &amp;\vdots &amp;\vdots &amp;\vdots \\ x^{(1)} &amp;x^{(2)} &amp;x^{(3)} &amp;\vdots \\ \vdots &amp;\vdots &amp;\vdots &amp;\vdots \end{bmatrix} &#x3D; \begin{bmatrix} \vdots &amp;\vdots &amp;\vdots &amp;\vdots \\ w^{(1)}x^{(1)} &amp;w^{(1)}x^{(2)} &amp;w^{(1)}x^{(3)} &amp;\vdots \\ \vdots &amp;\vdots &amp;\vdots &amp;\vdots \end{bmatrix} &#x3D; \begin{bmatrix} \vdots &amp;\vdots &amp;\vdots &amp;\vdots \\ z^{[1](1)} &amp;z^{[1](2)} &amp;z^{[1](3)} &amp;\vdots \\ \vdots &amp;\vdots &amp;\vdots &amp;\vdots \end{bmatrix} &#x3D; Z^{[1]}[&#x2F;latex] <strong>Stack up the training examples in the columns of matrix X, and their outputs are also stacked into the columns of matrix [latex]z^{[1]}[&#x2F;latex].</strong></p>
<h2 id="Activation-functions"><a href="#Activation-functions" class="headerlink" title="Activation functions"></a>Activation functions</h2><p>Algorithm 3.18 Sigmoid : [latex]a &#x3D; \sigma (z) &#x3D; \frac{1}{1 + e^{-z}}[&#x2F;latex] Algorithm 3.19 tanh : [latex]a &#x3D; \tanh (z) &#x3D; \frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}[&#x2F;latex] (almost always works better than the sigmoid function) Algorithm 3.20 hidden layer : [latex]g(z^{[1]}) &#x3D; tanh(z^{[1]})[&#x2F;latex] (almost always strictly superior) Algorithm 3.21 binaray : [latex]g(z^{[2]}) &#x3D; \sigma(z^{[2]})[&#x2F;latex] (if y is either 0 or 1) if z is either very large or very small then the gradient of the derivative or the slope of this function becomes very small so z is very large or z is very small the slope of the function you know ends up being close to zero and so this can slow down gradient descent Algorithm 3.22 Relu (Rectified Linear Unit) : [latex]a &#x3D; max(0, z)[&#x2F;latex] Algorithm 3.23 Leaky Relu: [latex]a &#x3D; max(0.01z, z)[&#x2F;latex] some rules of thumb for choosing activation functions :</p>
<ul>
<li>sigmoid : binary classification</li>
<li>tanh : pretty much strictly superior</li>
<li>ReLu : default</li>
</ul>
<p>If you’re not sure which one of these activation functions work best you know try them all and then evaluate on like a holdout validation set or like a development set which we’ll talk about later and see which one works better and then go with that.</p>
<h2 id="Why-need-a-nonlinear-activation-function"><a href="#Why-need-a-nonlinear-activation-function" class="headerlink" title="Why need a nonlinear activation function?"></a>Why need a nonlinear activation function?</h2><p>It turns out that for your neural network to compute interesting functions you do need to take a nonlinear activation function. It turns out that if you use a linear activation function or alternatively if you don’t have an activation function then no matter how many layers your neural network has always doing is just computing a linear activation function so you might as well not have any hidden layers.</p>
<h2 id="Derivatives-of-activation-functions"><a href="#Derivatives-of-activation-functions" class="headerlink" title="Derivatives of activation functions"></a>Derivatives of activation functions</h2><p>Algorithm 3.25 : [latex]\frac{\mathrm{d} }{\mathrm{d} z}g(z) &#x3D; \frac{1}{1+e^{-z}}(1 - \frac{1}{1+e^{-z}}) &#x3D; g(z)(1 - g(z))[&#x2F;latex] Algorithm 3.26 : [latex]g(z) &#x3D; \tanh (z) &#x3D; \frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}[&#x2F;latex] Algorithm 3.27 : [latex]\frac{\mathrm{d} }{\mathrm{d} z}g(z) &#x3D; 1 - (tanh(z))^2[&#x2F;latex] Algorithm Rectified Linear Unit (ReLU) : [latex]g(z)’ &#x3D; \left\{\begin{matrix} 0 &amp; if\ z &lt; 0\\ 1 &amp; if\ z &gt; 0\\ undefined &amp; if\ z &#x3D; 0 \end{matrix}\right.[&#x2F;latex] Algorithm Leaky linear unit (Leaky ReLU) : [latex]g(z)’ &#x3D; \left\{\begin{matrix} 0.01 &amp; if\ z &lt; 0\\ 1 &amp; if\ z &gt; 0\\ undefined &amp; if\ z &#x3D; 0 \end{matrix}\right.[&#x2F;latex]</p>
<h2 id="Gradient-descent-for-neural-networks"><a href="#Gradient-descent-for-neural-networks" class="headerlink" title="Gradient descent for neural networks"></a>Gradient descent for neural networks</h2><p>**forward propagation : ** (1) : [latex]z^{[1]} &#x3D; W^{[1]}x + b^{[1]}[&#x2F;latex] (2) : [latex]a^{[1]} &#x3D; \sigma(z^{[1]})[&#x2F;latex] (3) : [latex]z^{[2]} &#x3D; W^{[2]}a^{[1]} + b^{[2]}[&#x2F;latex] (4) : [latex]a^{[2]} &#x3D; g^{[2]}(z^{[2]}) &#x3D; \sigma(z^{[2]})[&#x2F;latex] **back propagation : ** Algorithm 3.32 : [latex]\mathrm{d}z^{[2]} &#x3D; A^{[2]} - Y, \ Y &#x3D; [y^{[1]} \ y^{[2]} \ \cdots \ y^{[m]}][&#x2F;latex] Algorithm 3.33 : [latex]\mathrm{d}W^{[2]} &#x3D; \frac{1}{m} \mathrm{d}z^{[2]}A^{[1]T}[&#x2F;latex] Algorithm 3.34 : [latex]\mathrm{d}b^{[2]} &#x3D; \frac{1}{m} np.sum(\mathrm{d}z^{[2]}, axis &#x3D; 1, keepdims &#x3D; True)[&#x2F;latex] Algorithm 3.35 : [latex]\mathrm{d}z^{[1]} &#x3D; \underbrace{W^{[2]T}\mathrm{d}z^{[2]}} * \underbrace{g^{[1]^{‘}}} * \underbrace{z^{[1]}}[&#x2F;latex] Algorithm 3.36 : [latex]\mathrm{d}W^{[1]} &#x3D; \frac{1}{m}\mathrm{d}z^{[1]}x^T[&#x2F;latex] Algorithm 3.37 : [latex]\underbrace{\mathrm{d}b^{[1]}} &#x3D; \frac {1}{m} np.sum(\mathrm{d}z^{[1]}, axis &#x3D; 1, keepdims &#x3D; True)[&#x2F;latex] (axis &#x3D; 1 : horizontally, keepdims : ensures that Python outputs, for d b^[2] a vector that is some n by one)</p>
<h2 id="Backpropagation-intuition"><a href="#Backpropagation-intuition" class="headerlink" title="Backpropagation intuition"></a>Backpropagation intuition</h2><p>It is one of the very hardest pieces of math. One of the very hardest derivations in all of machine learning.</p>
<p>&#x2F;&#x2F;TODO, maybe never …</p>
<h2 id="Random-Initialization"><a href="#Random-Initialization" class="headerlink" title="Random Initialization"></a>Random Initialization</h2><p><strong>It is important to initialize the weights randomly.</strong></p>
<ol>
<li>Gaussian random variable (2,2) : [latex]W^{[1]} &#x3D; np.random.randn(2, 2)[&#x2F;latex]</li>
<li>then usually you multiply this by a very small number such as 0.01 so you initialize it to very small random values</li>
</ol>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2019/04/10/routes-stream-to-specified-buffer/" rel="prev" title="Routes stream to specified buffer">
      <i class="fa fa-chevron-left"></i> Routes stream to specified buffer
    </a></div>
      <div class="post-nav-item">
    <a href="/2019/04/17/4-deep-neural-networks/" rel="next" title="4 Deep Neural Networks">
      4 Deep Neural Networks <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Neural-Network-Overview"><span class="nav-number">1.</span> <span class="nav-text">Neural Network Overview</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Neural-Network-Representation"><span class="nav-number">2.</span> <span class="nav-text">Neural Network Representation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Computing-a-Neural-Network%E2%80%99s-output"><span class="nav-number">3.</span> <span class="nav-text">Computing a Neural Network’s output</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Vectorizing-across-multiple-examples"><span class="nav-number">4.</span> <span class="nav-text">Vectorizing across multiple examples</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Justification-for-vectorized-implementation"><span class="nav-number">5.</span> <span class="nav-text">Justification for vectorized implementation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Activation-functions"><span class="nav-number">6.</span> <span class="nav-text">Activation functions</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Why-need-a-nonlinear-activation-function"><span class="nav-number">7.</span> <span class="nav-text">Why need a nonlinear activation function?</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Derivatives-of-activation-functions"><span class="nav-number">8.</span> <span class="nav-text">Derivatives of activation functions</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Gradient-descent-for-neural-networks"><span class="nav-number">9.</span> <span class="nav-text">Gradient descent for neural networks</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Backpropagation-intuition"><span class="nav-number">10.</span> <span class="nav-text">Backpropagation intuition</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Random-Initialization"><span class="nav-number">11.</span> <span class="nav-text">Random Initialization</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Water</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">216</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">64</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/linmonsv" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;linmonsv" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:qin2@qq.com" title="E-Mail → mailto:qin2@qq.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2017 – 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Water</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
