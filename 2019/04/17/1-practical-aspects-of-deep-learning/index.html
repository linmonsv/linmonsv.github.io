<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="Train &#x2F; Dev &#x2F; Test setsApplied deep learning is a very iterative process.  In the previous era of machine learning : the 70&#x2F;30 train test splits, if you don’t have an explicit dev set o">
<meta property="og:type" content="article">
<meta property="og:title" content="1 Practical aspects of Deep Learning">
<meta property="og:url" content="http://example.com/2019/04/17/1-practical-aspects-of-deep-learning/index.html">
<meta property="og:site_name" content="Water&#39;s Home">
<meta property="og:description" content="Train &#x2F; Dev &#x2F; Test setsApplied deep learning is a very iterative process.  In the previous era of machine learning : the 70&#x2F;30 train test splits, if you don’t have an explicit dev set o">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2019-04-17T06:59:41.000Z">
<meta property="article:modified_time" content="2022-07-04T07:26:09.539Z">
<meta property="article:author" content="Water">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/2019/04/17/1-practical-aspects-of-deep-learning/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>1 Practical aspects of Deep Learning | Water's Home</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Water's Home</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Just another Life Style</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2019/04/17/1-practical-aspects-of-deep-learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Water">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Water's Home">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          1 Practical aspects of Deep Learning
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-04-17 14:59:41" itemprop="dateCreated datePublished" datetime="2019-04-17T14:59:41+08:00">2019-04-17</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-07-04 15:26:09" itemprop="dateModified" datetime="2022-07-04T15:26:09+08:00">2022-07-04</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/machine-learning/" itemprop="url" rel="index"><span itemprop="name">machine-learning</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/machine-learning/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/deep-learning/" itemprop="url" rel="index"><span itemprop="name">deep-learning</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/deep-learning/Deep-Learning-Specialization-Offered-By-deeplearning-ai/" itemprop="url" rel="index"><span itemprop="name">Deep Learning Specialization Offered By deeplearning.ai</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="Train-x2F-Dev-x2F-Test-sets"><a href="#Train-x2F-Dev-x2F-Test-sets" class="headerlink" title="Train &#x2F; Dev &#x2F; Test sets"></a>Train &#x2F; Dev &#x2F; Test sets</h2><p>Applied deep learning is a very iterative process.</p>
<ul>
<li>In the previous era of machine learning : the 70&#x2F;30 train test splits, if you don’t have an explicit dev set or maybe a 60&#x2F;20&#x2F;20% split</li>
<li>In the modern big data era : 100w examples, 98&#x2F;1&#x2F;1 or 99.5&#x2F;0.25&#x2F;0.25</li>
<li>Make sure that the dev and test sets come from the same distribution</li>
<li>It might be okay to not have a test set. <em>The goal of the test set is to give you a unbiased estimate</em> <em>of the performance of your final network, of the network that you selected. But if you don’t need that unbiased estimate, then it might be okay to not have a test set.</em></li>
</ul>
<h2 id="Bias-x2F-Variance"><a href="#Bias-x2F-Variance" class="headerlink" title="Bias &#x2F; Variance"></a>Bias &#x2F; Variance</h2><ul>
<li>High Bias : not a very good fit to the data what we say that this is underfitting the data.</li>
<li>High Variance : this is overfitting the data and would not generalizing well.</li>
<li>The optimal error, sometimes called Bayesian error.</li>
</ul>
<p>How to analyze bias and variance when no classifier can do very well :</p>
<ul>
<li>Get a sense of how well you are fitting by looking at your training set error</li>
<li>Go to the dev set and look at how bad is the variance problem</li>
</ul>
<h2 id="Basic-Recipe-for-Machine-Learning"><a href="#Basic-Recipe-for-Machine-Learning" class="headerlink" title="Basic Recipe for Machine Learning"></a>Basic Recipe for Machine Learning</h2><ol>
<li>Does your algorithm have high bias? And so to try and evaluate if there is high bias, And so, if it does not even fit in the training set that well, some things you could try would be to try pick a network.</li>
<li>Maybe you can make it work, maybe not, whereas getting a bigger network almost always helps. And training longer doesn’t always help, but it certainly never hurts. Try these things until I can at least get rid of the bias problems, as in go back after I’ve tried this and keep doing that until I can fit, at least, fit the training set pretty well.</li>
<li>Once you reduce bias to a acceptable amounts, then ask, do you have a variance problem?</li>
<li>And if you have high variance, well, best way to solve a high variance problem is to get more data. But sometimes you can’t get more data. Or you could try regularization.</li>
</ol>
<p><strong>Repeat until hopefully you find something with both low bias and low variance.</strong> <strong>Notes :</strong></p>
<ul>
<li>If you actually have a high bias problem, getting more training data is actually not going to help.</li>
<li>Getting a bigger network almost always just reduces your bias without necessarily hurting your variance, so long as you regularize appropriately. And getting more data pretty much always reduces your variance and doesn’t hurt your bias much.</li>
</ul>
<p>Training a bigger network almost never hurts. And the main cost of training a neural network that’s too big is just computational time, so long as you’re regularizing.</p>
<h2 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h2><p><strong>High Variance Problem :</strong></p>
<ul>
<li>probably regularization</li>
<li>get more training data</li>
</ul>
<p>Regularization will often help to prevent overfitting, or to reduce the errors in your network. Add regularization to the logistic regression, what you do is add [latex]\lambda[&#x2F;latex] to it, which is called the Regularization Parameter.</p>
<ul>
<li>L2 regularization (the most common type of regularization) [latex]J(w,b) &#x3D; \frac {1}{m} \sum _{i&#x3D;1}^{m} L(\hat y ^{(i)}, y ^{(i)}) + \frac {\lambda}{2m} \left \ w \right \ ^{2}_{2}[&#x2F;latex]</li>
<li>L1 regularization [latex][&#x2F;latex]</li>
</ul>
<p><strong>Frobenius norm :</strong> (L2 normal of a matrix) It just means the sum of square of elements of a matrix. L2 regularization is sometimes also called weight decay.</p>
<h2 id="Why-regularization-reduces-overfitting"><a href="#Why-regularization-reduces-overfitting" class="headerlink" title="Why regularization reduces overfitting?"></a>Why regularization reduces overfitting?</h2><p>One piece of intuition is that if you crank regularisation lambda to be really, really big, they’ll be really incentivized to set the weight matrices W to be reasonably close to zero. So one piece of intuition is maybe it set the weight to be so close to zero for a lot of hidden units that’s basically zeroing out a lot of the impact of these hidden units.</p>
<h2 id="Dropout-Regularization"><a href="#Dropout-Regularization" class="headerlink" title="Dropout Regularization"></a>Dropout Regularization</h2><p>With dropout, what we’re going to do is go through each of the layers of the network, and set some probability of eliminating a node in neural network. So you end up with a much smaller, really much diminished network. And then you do back propagation training. By far the most common implementation of dropouts today is inverted dropouts.</p>
<h2 id="Understanding-Dropout"><a href="#Understanding-Dropout" class="headerlink" title="Understanding Dropout"></a>Understanding Dropout</h2><ul>
<li>So it’s as if on every iteration, you’re working with a smaller neural network, and so using a smaller neural network seems like it should have a regularizing effect.</li>
<li>Similar to what we saw with L2 regularization, the effect of implementing dropout is that it shrinks the weights, and does some of those outer regularization that helps prevent over-fitting.</li>
</ul>
<p><strong>Notice that the keep_prob of one point zero means that you’re keeping every unit</strong> If you’re more worried about some layers overfitting than others, you can set a lower keep_prob for some layers than others. The downside is, this gives you even more hyper parameters to search for using cross-validation. One other alternative might be to have some layers where you apply dropout and some layers where you don’t apply dropout and then just have one hyper parameter, which is the keep_prob for the layers for which you do apply dropout. On computer vision, the input size is so big, you inputting all these pixels that you almost never have enough data. So you’re almost always overfitting, And so dropout is very frequently used by computer vision. One big downside of dropout is that the cost function J is no longer well-defined. On every iteration, you are randomly killing off a bunch of nodes. and so, if you are double checking the performance of gradient dissent, it’s actually harder to double check that right, you have a well-defined cost function J that is going downhill on every iteration.</p>
<h2 id="Other-regularization-methods"><a href="#Other-regularization-methods" class="headerlink" title="Other regularization methods"></a>Other regularization methods</h2><ul>
<li>data augmentation : flipping it horizontally, random rotations and distortions</li>
<li>early stopping : <ol>
<li>And the advantage of early stopping is that running the gradient descent process just once, you get to try out values of small w, mid-size w, and large w, without needing to try a lot of values of the L2 regularization hyperparameter lambda.</li>
<li>The Problem is that because of stopping gradient descent eailer, so that not doing a great job reducing the cost function J. And then you also trying to not over fit.</li>
</ol>
</li>
</ul>
<h2 id="Normalizing-inputs"><a href="#Normalizing-inputs" class="headerlink" title="Normalizing inputs"></a>Normalizing inputs</h2><p>When training a neural network, one of the techniques that will <strong>speed up</strong> your training. Normalizing your inputs corresponds to two steps : </p>
<ol>
<li>subtract out or to zero out the mean</li>
<li>normalize the variances</li>
</ol>
<p>If your features came in on similar scales, then this step is less important, although performing this type of normalization pretty much never does any harm, so I’ll often do it anyway if I’m not sure whether or not it will help with speeding up training for your algorithm.</p>
<h2 id="Vanishing-x2F-Exploding-gradients"><a href="#Vanishing-x2F-Exploding-gradients" class="headerlink" title="Vanishing &#x2F; Exploding gradients"></a>Vanishing &#x2F; Exploding gradients</h2><p>When you’re training a very deep network, your derivatives or your slopes can sometimes get either very very big or very very small, maybe even exponentially small, and this makes training difficult. Use careful choices of the random weight initialization to significantly reduce this problem.</p>
<h2 id="Weight-Initialization-for-Deep-Networks"><a href="#Weight-Initialization-for-Deep-Networks" class="headerlink" title="Weight Initialization for Deep Networks"></a>Weight Initialization for Deep Networks</h2><p>More careful choice of the random initialization for your neural network. Some formulas gives a default value to use for the variance of the initialization of weight matrices :</p>
<ul>
<li>tanh : Xavier initialization [latex]\sqrt{\frac{1}{n^{[l-1]}}}[&#x2F;latex], or [latex]\sqrt{\frac{2}{n^{[l-1]}+n^{[l]}}}[&#x2F;latex]</li>
<li>Relu : [latex]\sqrt{\frac{2}{n^{[l-1]}}}[&#x2F;latex]</li>
</ul>
<h2 id="Numerical-approximation-of-gradients"><a href="#Numerical-approximation-of-gradients" class="headerlink" title="Numerical approximation of gradients"></a>Numerical approximation of gradients</h2><p>When you implement back propagation you’ll find that there’s a test called <strong>gradient checking</strong> that can really help you <strong>make sure</strong> that your implementation of <strong>back prop is correct</strong>. Because sometimes you write all these equations and you’re just not 100% sure if you’ve got all the details right and implementing back propagation. So <strong>in order to build up to gradient checking</strong>, let’s first talk about how to <strong>numerically approximate computations of gradients</strong>. How to numerically approximate computations of gradients The formal definition of a derivative : [latex]f’(\theta) &#x3D; \frac {f(\theta + \varepsilon) - f(\theta - \varepsilon)}{2\varepsilon }[&#x2F;latex]</p>
<h2 id="Gradient-checking"><a href="#Gradient-checking" class="headerlink" title="Gradient checking"></a>Gradient checking</h2><p>How you could use it too to debug, or to verify that your implementation and back props correct. [latex]\mathrm{d} \theta _{approx}[i] &#x3D; \frac {J(\theta_1, \theta_2, \cdots \theta_i + \varepsilon, \cdots) - J(\theta_1, \theta_2, \cdots \theta_i - \varepsilon, \cdots)}{2\varepsilon }[&#x2F;latex]   [latex]\frac{\left \ \mathrm{d} \theta _{approx}[i] - \mathrm{d} \theta [i] \right \_2}{\left \ \mathrm{d} \theta _{approx}[i] \right \_2 + \left \ \mathrm{d} \theta [i] \right \_2} &#x3D; \varepsilon \left\{\begin{matrix} &lt; 10^{-7} &amp; , that’s great\\ &gt; 10^{-5} &amp; , maybe have a bug somewhere \end{matrix}\right.[&#x2F;latex]  </p>
<h2 id="Gradient-Checking-Implementation-Notes"><a href="#Gradient-Checking-Implementation-Notes" class="headerlink" title="Gradient Checking Implementation Notes"></a>Gradient Checking Implementation Notes</h2><ol>
<li>Don’t use in training - only to debug</li>
<li>If algorithm fails grad check , look at components to try to identify bug.</li>
<li>Remember regularization</li>
<li>Doesn’t work with dropout</li>
<li>Run at random initialization; perhaps again after some training.</li>
</ol>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2019/04/17/4-deep-neural-networks/" rel="prev" title="4 Deep Neural Networks">
      <i class="fa fa-chevron-left"></i> 4 Deep Neural Networks
    </a></div>
      <div class="post-nav-item">
    <a href="/2019/04/17/2-optimization-algorithms/" rel="next" title="2 Optimization algorithms">
      2 Optimization algorithms <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Train-x2F-Dev-x2F-Test-sets"><span class="nav-number">1.</span> <span class="nav-text">Train &#x2F; Dev &#x2F; Test sets</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Bias-x2F-Variance"><span class="nav-number">2.</span> <span class="nav-text">Bias &#x2F; Variance</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Basic-Recipe-for-Machine-Learning"><span class="nav-number">3.</span> <span class="nav-text">Basic Recipe for Machine Learning</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Regularization"><span class="nav-number">4.</span> <span class="nav-text">Regularization</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Why-regularization-reduces-overfitting"><span class="nav-number">5.</span> <span class="nav-text">Why regularization reduces overfitting?</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Dropout-Regularization"><span class="nav-number">6.</span> <span class="nav-text">Dropout Regularization</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Understanding-Dropout"><span class="nav-number">7.</span> <span class="nav-text">Understanding Dropout</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Other-regularization-methods"><span class="nav-number">8.</span> <span class="nav-text">Other regularization methods</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Normalizing-inputs"><span class="nav-number">9.</span> <span class="nav-text">Normalizing inputs</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Vanishing-x2F-Exploding-gradients"><span class="nav-number">10.</span> <span class="nav-text">Vanishing &#x2F; Exploding gradients</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Weight-Initialization-for-Deep-Networks"><span class="nav-number">11.</span> <span class="nav-text">Weight Initialization for Deep Networks</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Numerical-approximation-of-gradients"><span class="nav-number">12.</span> <span class="nav-text">Numerical approximation of gradients</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Gradient-checking"><span class="nav-number">13.</span> <span class="nav-text">Gradient checking</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Gradient-Checking-Implementation-Notes"><span class="nav-number">14.</span> <span class="nav-text">Gradient Checking Implementation Notes</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Water</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">216</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">64</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/linmonsv" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;linmonsv" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:qin2@qq.com" title="E-Mail → mailto:qin2@qq.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2017 – 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Water</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
