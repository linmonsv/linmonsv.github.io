<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="Why ML Strategy?How to improve your system :   more data diverse poses   negative examples train the algorithm longer try a different optimization algorithm trying a bigger network or a smaller networ">
<meta property="og:type" content="article">
<meta property="og:title" content="1 ML strategy (1)">
<meta property="og:url" content="http://example.com/2019/04/18/1-ml-strategy-1/index.html">
<meta property="og:site_name" content="Water&#39;s Home">
<meta property="og:description" content="Why ML Strategy?How to improve your system :   more data diverse poses   negative examples train the algorithm longer try a different optimization algorithm trying a bigger network or a smaller networ">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2019-04-18T06:15:07.000Z">
<meta property="article:modified_time" content="2022-07-04T07:26:09.546Z">
<meta property="article:author" content="Water">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/2019/04/18/1-ml-strategy-1/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>1 ML strategy (1) | Water's Home</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Water's Home</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Just another Life Style</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2019/04/18/1-ml-strategy-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Water">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Water's Home">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          1 ML strategy (1)
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-04-18 14:15:07" itemprop="dateCreated datePublished" datetime="2019-04-18T14:15:07+08:00">2019-04-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-07-04 15:26:09" itemprop="dateModified" datetime="2022-07-04T15:26:09+08:00">2022-07-04</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/machine-learning/" itemprop="url" rel="index"><span itemprop="name">machine-learning</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/machine-learning/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/deep-learning/" itemprop="url" rel="index"><span itemprop="name">deep-learning</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/deep-learning/Deep-Learning-Specialization-Offered-By-deeplearning-ai/" itemprop="url" rel="index"><span itemprop="name">Deep Learning Specialization Offered By deeplearning.ai</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="Why-ML-Strategy"><a href="#Why-ML-Strategy" class="headerlink" title="Why ML Strategy?"></a>Why ML Strategy?</h2><p><strong>How to improve your system :</strong> </p>
<ul>
<li>more data</li>
<li>diverse<ul>
<li>poses</li>
</ul>
</li>
<li>negative examples</li>
<li>train the algorithm longer</li>
<li>try a different optimization algorithm<ul>
<li>trying a bigger network or a smaller network</li>
<li>try to dropout or maybe L2 regularization</li>
<li>change the network architecture</li>
<li>changing activation functions</li>
<li>changing the number of hidden units and so on</li>
</ul>
</li>
</ul>
<p>And the problem is that if you choose poorly, it is entirely possible that you end up spending six months charging in some direction only to realize after six months that that didnâ€™t do any good. So we <strong>need a number of strategies</strong>, that is, ways of analyzing a machine learning problem that will point you in the direction of the most promising things to try.</p>
<h2 id="Orthogonalization"><a href="#Orthogonalization" class="headerlink" title="Orthogonalization"></a>Orthogonalization</h2><p>You must be very clear-eyed about what to tune in order to try to achieve one effect. This is a process we call orthogonalization. So the concept of orthogonalization refers to that, if you think of one dimension of what you want to do as controlling a steering angle, and another dimension as controlling your speed. And by having orthogonal, orthogonal means at 90 degrees to each other. By having orthogonal controls that are ideally aligned with the things you actually want to control, it makes it much easier to tune the knobs you have to tune.</p>
<h4 id="Chain-of-assumptions-in-ML"><a href="#Chain-of-assumptions-in-ML" class="headerlink" title="Chain of assumptions in ML"></a>Chain of assumptions in ML</h4><ul>
<li>Fit training set well on cost function</li>
<li>Fit dev set well on cost function</li>
<li>Fit test set well on cost function</li>
<li>Performs well in real world</li>
</ul>
<h2 id="Single-number-evaluation-metric"><a href="#Single-number-evaluation-metric" class="headerlink" title="Single number evaluation metric"></a>Single number evaluation metric</h2><p>Whether youâ€™re tuning hyperparameters, or trying out different ideas for learning algorithms, or just trying out different options for building your machine learning system. Youâ€™ll find that your progress will be <strong>much faster if you have</strong> a single real number evaluation metric that lets you quickly tell if the new thing you just tried is working better or worse than your last idea. One reasonable way to evaluate the performance of your classifiers is to look at its precision and recall. It turns out that thereâ€™s often a tradeoff between precision and recall, and you care about both. [latex]F_1 &#x3D; \frac{2}{\frac{1}{P} + \frac{1}{R}}[&#x2F;latex] And in mathematics, this function is called the harmonic mean of precision P and recall R. A well-defined dev setÂ which is how youâ€™re measuring precision and recall,Â plus a <strong>single number evaluation metric</strong> allows you to quickly tell if classifier A or classifier B is better,</p>
<h2 id="Satisficing-and-optimizing-metrics"><a href="#Satisficing-and-optimizing-metrics" class="headerlink" title="Satisficing and optimizing metrics"></a>Satisficing and optimizing metrics</h2><p>Itâ€™s not always easy to combine all the things you care about into a single real number evaluation metric. In those cases it sometimes useful to set up satisficing as well as optimizing metrics. If you have N metrics that you care about itâ€™s sometimes reasonable to <strong>pick one of them to be optimizing</strong>. So you want to do as well as is possible on that one. And then N minus 1 to be <strong>satisficing</strong>, meaning that so long as they reach some threshold. Such as running times faster than 100 milliseconds, but <strong>so long as they reach some threshold, you donâ€™t care how much better it is in that threshold, but they have to reach that threshold</strong>. If there are multiple things you care about by say thereâ€™s one as the optimizing metric that you want to do as well as possible on and one or more as satisficing metrics were youâ€™ll be satisfice. Almost it does better than some threshold you can now have an almost automatic way of quickly looking at multiple cost size and picking the, quote, best one.</p>
<h2 id="Train-x2F-dev-x2F-test-distributions"><a href="#Train-x2F-dev-x2F-test-distributions" class="headerlink" title="Train&#x2F;dev&#x2F;test distributions"></a>Train&#x2F;dev&#x2F;test distributions</h2><p>The <strong>dev set</strong> is also called the <strong>development set</strong>, or sometimes called the hold out <strong>cross validation set</strong>. <strong>Make your dev and test sets come from the same distribution.</strong> Take all this data, randomly shuffled data into the dev and test set. So that, both the dev and test sets have data from all eight regions and that the dev and test sets really come from the same distribution, Machine learning teams are often very good at shooting different arrows into targets and iterating to get closer and closer to hitting the bullseye. Once you do well on, try to get data that looks like that. And, whatever that data is, put it into both your dev set and your test set. A totally different location that just was a very frustrating experience for the team. <strong>Setting up the dev set, as well as the evaluation metric, is really defining what target you want to aim at.</strong></p>
<h2 id="Size-of-dev-and-test-sets"><a href="#Size-of-dev-and-test-sets" class="headerlink" title="Size of dev and test sets"></a>Size of dev and test sets</h2><ul>
<li>train and test set : 70&#x2F;30</li>
<li>train dev and test sets : 60&#x2F;20&#x2F;20</li>
<li>train dev and test sets : 98&#x2F;1&#x2F;1</li>
</ul>
<h4 id="Size-of-test-set"><a href="#Size-of-test-set" class="headerlink" title="Size of test set"></a>Size of test set</h4><p>Set your test set to be big enough to give high condifience in the over all performance of your system.</p>
<h2 id="When-to-change-dev-x2F-test-sets-and-metrics"><a href="#When-to-change-dev-x2F-test-sets-and-metrics" class="headerlink" title="When to change dev&#x2F;test sets and metrics"></a>When to change dev&#x2F;test sets and metrics</h2><p>Sometimes partway through a project you might realize you put your target in the wrong place. In that case you should move your target. Misclassification error metric : [latex]Error &#x3D; \frac{1}{m_{dev}} \sum _{i&#x3D;1}^{m_{dev}}I\{y_{pred}^{(i)} \neq y^{(i)}\}[&#x2F;latex] One way to change this evaluation metric : [latex]Error &#x3D; \frac{1}{m_{dev}} \sum _{i&#x3D;1}^{m_{dev}}w^{(i)}I\{y_{pred}^{(i)} \neq y^{(i)}\}[&#x2F;latex] If you want this normalization constant, technically this becomes sum over i of w(i), so then this error would still be between zero and one.Â [latex]Error &#x3D; \frac{1}{\sum w^{(i)}} \sum _{i&#x3D;1}^{m_{dev}}w^{(i)}I\{y_{pred}^{(i)} \neq y^{(i)}\}[&#x2F;latex] <strong>The goal of the evaluation metric is accurately tell you, given two classifiers, which one is better for your application.</strong> If youâ€™re not satisfied with your old error metric then donâ€™t keep coasting with an error metric youâ€™re unsatisfied with, instead try to define a new one that you think better captures your preferences in terms of whatâ€™s actually a better algorithm. <strong>Take a machine learning problem and break it into distinct steps.</strong></p>
<ul>
<li>place the target</li>
<li>shooting at the target</li>
</ul>
<p><strong>The point was with the philosophy of orthogonalization.</strong> If doing well on your metric and your current dev sets or dev and test setsâ€™ distribution, if that does not correspond to doing well on the application you actually care about, then change your metric and your dev test set. <strong>The overall guideline is if your current metric and data you are evaluating on doesnâ€™t correspond to doing well on what you actually care about, then change your metrics and&#x2F;or your dev&#x2F;test set to better capture what you need your algorithm to actually do well on.</strong> <strong>Even if you canâ€™t define the perfect evaluation metric and dev set, just set something up quickly and use that to drive the speed of your team iterating.</strong> And if later down the line you find out that it wasnâ€™t a good one, you have better idea, change it at that time, itâ€™s perfectly okay.</p>
<h2 id="Why-human-level-performance"><a href="#Why-human-level-performance" class="headerlink" title="Why human-level performance?"></a>Why human-level performance?</h2><ol>
<li>In deep learning, machine learning algorithms are suddenly working much better and so it has become much more feasible in a lot of application areas for machine learning algorithms to actually become competitive with human-level performance.</li>
<li>The workflow of designing and building a machine learning system, the workflow is much more efficient when youâ€™re trying to do something that humans can also do.</li>
</ol>
<p>And over time, as you keep training the algorithm, maybe bigger and bigger models on more and more data, the performance approaches but never surpasses some theoretical limit, which is called the <strong>Bayes optimal error</strong>. So Bayes optimal error, think of this as the best possible error. And Bayes optimal error, or Bayesian optimal error, or sometimes Bayes error for short, is the very best theoretical function for mapping from x to y. <strong>That can never be surpassed</strong>. <strong>It turns out that progress is often quite fast until you surpass human level performance. And it sometimes slows down after you surpass human level performance.</strong></p>
<ol>
<li>One reason is that human level performance is for many tasks not that far from Bayesâ€™ optimal error.</li>
<li>so long as your performance is worse than human level performance, then there are actually certain tools you could use to improve performance that are harder to use once youâ€™ve surpassed human level performance.<ul>
<li>For tasks that humans are good at, so long as your machine learning algorithm is still worse than the human, you can get labeled data from humans. That is you can ask people, ask or hire humans, to label examples for you so that you can have more data to feed your learning algorithm.</li>
</ul>
</li>
</ol>
<p><strong>Knowing how well humans can do well on a task can help you understand better how much you should try to reduce bias and how much you should try to reduce variance.</strong></p>
<h2 id="Avoidable-bias"><a href="#Avoidable-bias" class="headerlink" title="Avoidable bias"></a>Avoidable bias</h2><p>If thereâ€™s a <strong>huge gap between how well your algorithm does on your training set versus how humans do</strong> shows that your algorithm isnâ€™t even fitting the training set well.So in terms of tools to reduce bias or variance, in this case I would say focus on <strong>reducing bias</strong>. In another case, even though your training error and dev error are the same as the other example, you see that maybe youâ€™re actually doing just fine on the training set. Itâ€™s doing only <strong>a little bit worse than human level performance</strong>. You would maybe want to focus on reducing this component, <strong>reducing the variance</strong> in your learning algorithm. Think of <strong>human level error as a proxy or as a estimate for Bayes error</strong> or for Bayes optimal error. And <strong>for computer vision tasks, this is a pretty reasonable proxy</strong> because humans are actually very good at computer vision and so whatever a human can do is maybe not too far from Bayes error. The difference between Bayes error or approximation of Bayes error and the training error is the <strong>avoidable bias</strong>. The difference between your training area and the dev error, thereâ€™s a measure still of the variance problem of your algorithm.</p>
<h2 id="Understanding-human-level-performance"><a href="#Understanding-human-level-performance" class="headerlink" title="Understanding human-level performance"></a>Understanding human-level performance</h2><p>Human-level error, is that it gives us a way of estimating Bayes error. What is the best possible error any function could, either now or in the future.</p>
<h4 id="How-should-you-define-human-level-error"><a href="#How-should-you-define-human-level-error" class="headerlink" title="How should you define human-level error?"></a>How should you define human-level error?</h4><p><strong>To be clear about what your purpose is in defining the term human-level error.</strong></p>
<p>This gap between Bayes error or estimate of Bayes error and training error is calling that a measure of the avoidable bias. And this as a measure or an estimate of how much of a variance problem you have in your learning algorithm.</p>
<ul>
<li>The difference between your estimate of Bayes error tells you how much avoidable bias is a problem, how much avoidable bias there is.</li>
<li>And the difference between training error and dev error, that tells you how much variance is a problem, whether your algorithmâ€™s able to generalize from the training set to the dev set.</li>
</ul>
<p><strong>A better estimate for Bayes error can help you better estimate avoidable bias and variance. And therefore make better decisions on whether to focus on bias reduction tactics, or on variance reduction tactics.</strong></p>
<h2 id="Surpassing-human-level-performance"><a href="#Surpassing-human-level-performance" class="headerlink" title="Surpassing human- level performance"></a>Surpassing human- level performance</h2><p><strong>Surpassing human-level performance :</strong> </p>
<ul>
<li>Team of humans</li>
<li>One human</li>
<li>Training error</li>
<li>Dev error</li>
</ul>
<p>If your error is already better than even a team of humans looking at and discussing and debating the right label, then itâ€™s just also harder to rely on human intuition to tell your algorithm what are ways that your algorithm could still improve the performance <strong>Humans tend to be very good in natural perception task</strong>. So it is possible, but itâ€™s just a bit harder for computers to surpass human-level performance on natural perception task. <strong>Problems where ML significantly surpasses human-level performance :</strong></p>
<ul>
<li>Online advertising</li>
<li>Product recommendations</li>
<li>Logistics (predicting transit time)</li>
<li>Loan approvals</li>
</ul>
<p>And finally, all of these are problems where there are teams that have access to huge amounts of data. So for example, the best systems for all four of these applications have <strong>probably looked at far more data</strong> of that application than any human could possibly look at. And so, thatâ€™s also made it relatively easy for a computer to surpass human-level performance.</p>
<h2 id="Improving-your-model-performance"><a href="#Improving-your-model-performance" class="headerlink" title="Improving your model performance"></a>Improving your model performance</h2><p><strong>The two fundamental assumptions of supervised learning</strong></p>
<ul>
<li>You can fit the training set pretty well</li>
<li>The training set performance generalizes pretty well to the dev&#x2F;test set</li>
</ul>
<p><strong>Reducing (avoidable) bias and variance</strong></p>
<ul>
<li>Human-level<ul>
<li>Train bigger model</li>
<li>Train longer&#x2F;better optimization algorithms</li>
<li>NN architecture&#x2F;hyperparameters search</li>
</ul>
</li>
<li>Training error<ul>
<li>More data</li>
<li>Regularization</li>
<li>NN architecture&#x2F;hyperparameters search</li>
</ul>
</li>
<li>Dev error</li>
</ul>
<p><strong>This notion of bias or avoidable bias and varianceÂ there is one of those things that easily learned, but tough to master</strong></p>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2019/04/18/3-hyperparameter-tuning/" rel="prev" title="3 Hyperparameter tuning">
      <i class="fa fa-chevron-left"></i> 3 Hyperparameter tuning
    </a></div>
      <div class="post-nav-item">
    <a href="/2019/04/18/2-ml-strategy-2/" rel="next" title="2 ML strategy (2)">
      2 ML strategy (2) <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Why-ML-Strategy"><span class="nav-number">1.</span> <span class="nav-text">Why ML Strategy?</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Orthogonalization"><span class="nav-number">2.</span> <span class="nav-text">Orthogonalization</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Chain-of-assumptions-in-ML"><span class="nav-number">2.0.1.</span> <span class="nav-text">Chain of assumptions in ML</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Single-number-evaluation-metric"><span class="nav-number">3.</span> <span class="nav-text">Single number evaluation metric</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Satisficing-and-optimizing-metrics"><span class="nav-number">4.</span> <span class="nav-text">Satisficing and optimizing metrics</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Train-x2F-dev-x2F-test-distributions"><span class="nav-number">5.</span> <span class="nav-text">Train&#x2F;dev&#x2F;test distributions</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Size-of-dev-and-test-sets"><span class="nav-number">6.</span> <span class="nav-text">Size of dev and test sets</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Size-of-test-set"><span class="nav-number">6.0.1.</span> <span class="nav-text">Size of test set</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#When-to-change-dev-x2F-test-sets-and-metrics"><span class="nav-number">7.</span> <span class="nav-text">When to change dev&#x2F;test sets and metrics</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Why-human-level-performance"><span class="nav-number">8.</span> <span class="nav-text">Why human-level performance?</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Avoidable-bias"><span class="nav-number">9.</span> <span class="nav-text">Avoidable bias</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Understanding-human-level-performance"><span class="nav-number">10.</span> <span class="nav-text">Understanding human-level performance</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#How-should-you-define-human-level-error"><span class="nav-number">10.0.1.</span> <span class="nav-text">How should you define human-level error?</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Surpassing-human-level-performance"><span class="nav-number">11.</span> <span class="nav-text">Surpassing human- level performance</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Improving-your-model-performance"><span class="nav-number">12.</span> <span class="nav-text">Improving your model performance</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Water</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">216</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">64</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/linmonsv" title="GitHub â†’ https:&#x2F;&#x2F;github.com&#x2F;linmonsv" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:qin2@qq.com" title="E-Mail â†’ mailto:qin2@qq.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2017 â€“ 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Water</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
