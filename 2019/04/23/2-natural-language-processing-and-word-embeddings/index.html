<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"always","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="Word RepresentationNLP, Natural Language Processing Word embeddings, which is a way of representing words. that let your algorithms automatically understand analogies like that, man is to woman, as ki">
<meta property="og:type" content="article">
<meta property="og:title" content="2 Natural Language Processing and Word Embeddings">
<meta property="og:url" content="http://example.com/2019/04/23/2-natural-language-processing-and-word-embeddings/index.html">
<meta property="og:site_name" content="Water&#39;s Home">
<meta property="og:description" content="Word RepresentationNLP, Natural Language Processing Word embeddings, which is a way of representing words. that let your algorithms automatically understand analogies like that, man is to woman, as ki">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/img/Word_representation.png">
<meta property="og:image" content="http://example.com/img/Visualizing_word_embeddings.png">
<meta property="og:image" content="http://example.com/img/Embedding_matrix.png">
<meta property="og:image" content="http://example.com/img/A_note_on_the_featurization_view_of_word_embeddings.png">
<meta property="og:image" content="http://example.com/img/RNN_for_sentiment_classification.png">
<meta property="article:published_time" content="2019-04-23T01:43:11.000Z">
<meta property="article:modified_time" content="2022-07-05T01:29:39.444Z">
<meta property="article:author" content="Water">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/Word_representation.png">

<link rel="canonical" href="http://example.com/2019/04/23/2-natural-language-processing-and-word-embeddings/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>2 Natural Language Processing and Word Embeddings | Water's Home</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Water's Home</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Just another Life Style</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2019/04/23/2-natural-language-processing-and-word-embeddings/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Water">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Water's Home">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          2 Natural Language Processing and Word Embeddings
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-04-23 09:43:11" itemprop="dateCreated datePublished" datetime="2019-04-23T09:43:11+08:00">2019-04-23</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-07-05 09:29:39" itemprop="dateModified" datetime="2022-07-05T09:29:39+08:00">2022-07-05</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/machine-learning/" itemprop="url" rel="index"><span itemprop="name">machine-learning</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/machine-learning/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/deep-learning/" itemprop="url" rel="index"><span itemprop="name">deep-learning</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/deep-learning/Deep-Learning-Specialization-Offered-By-deeplearning-ai/" itemprop="url" rel="index"><span itemprop="name">Deep Learning Specialization Offered By deeplearning.ai</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="Word-Representation"><a href="#Word-Representation" class="headerlink" title="Word Representation"></a>Word Representation</h2><p><strong>NLP, Natural Language Processing</strong> <strong>Word embeddings</strong>, which is a way of representing words. that let your algorithms automatically understand analogies like that, man is to woman, as king is to queen, and many other examples. <img src="/img/Word_representation.png"> <strong>Representing words using a vocabulary of words.</strong> </p>
<p>One of the weaknesses of this representation is that it treats each word as a thing onto itself, and it doesn’t allow an algorithm to easily generalize the cross words.</p>
<p><img src="/img/Visualizing_word_embeddings.png"> You see plots like these sometimes on the internet to visualize some of these 300 or higher dimensional embeddings. To visualize it, algorithms like <strong>t-SNE</strong>, map this to a much lower dimensional space.</p>
<h2 id="Using-Word-Embeddings"><a href="#Using-Word-Embeddings" class="headerlink" title="Using Word Embeddings"></a>Using Word Embeddings</h2><p><strong>Transfer learning and word embeddings</strong></p>
<ol>
<li>Learn word embeddings from large text corpus. (1-100B words or download pre-trained embedding online.)</li>
<li>Transfer embedding to new task with smaller training set. (say, 100k words)</li>
<li>Optional: Continue to finetune the word embeddings with new data.</li>
</ol>
<h2 id="Properties-of-Word-Embeddings"><a href="#Properties-of-Word-Embeddings" class="headerlink" title="Properties of Word Embeddings"></a>Properties of Word Embeddings</h2><p>One of the most fascinating properties of word embeddings is that they can also help with <strong>analogy reasoning</strong>. The most commonly used similarity function is called <strong>cosine similarity</strong> : [latex]CosineSimilarity(u,v) &#x3D; \frac{u.v}{\left \ u \right \_2\left \ v \right \_2} &#x3D; cos(\theta)[&#x2F;latex]</p>
<h2 id="Embedding-Matrix"><a href="#Embedding-Matrix" class="headerlink" title="Embedding Matrix"></a>Embedding Matrix</h2><p>When you <strong>implement an algorithm to learn a word embedding</strong>, what you end up learning is an embedding matrix. And the <strong>columns</strong> of this matrix would be <strong>the different embeddings for the 10,000 different words</strong> you have in your vocabulary. <img src="/img/Embedding_matrix.png"></p>
<h2 id="Learning-Word-Embeddings"><a href="#Learning-Word-Embeddings" class="headerlink" title="Learning Word Embeddings"></a>Learning Word Embeddings</h2><p>It turns out that <strong>building a neural language model</strong> is a reasonable way to learn a set of embedding. Well, what’s actually more commonly done is to have a <strong>fixed historical window</strong>.</p>
<p><em>And using a fixed history, just means that you can deal with even arbitrarily long sentences because the input sizes are always fixed.</em></p>
<p>If your goal is to learn a embedding. Researchers have experimented with many different types of context.</p>
<ul>
<li>If your goal is to build a language model then it is natural for the context to be a few words right before the target word.</li>
<li>But if your goal isn’t to learn the language model per se, then you can choose other contexts.</li>
</ul>
<h2 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h2><p>The Word2Vec algorithm which is simple and computationally more efficient way to learn this types of embeddings.</p>
<h4 id="Skip-Gram-model"><a href="#Skip-Gram-model" class="headerlink" title="Skip-Gram model"></a>Skip-Gram model</h4><p>[latex]\begin{matrix} Softmax : &amp; p(tc) &#x3D; \frac{e^{\theta ^T_t e_c}}{\sum _{j&#x3D;1}^{10,000} e^{\theta ^T_j e_c}} \\ Loss Function : &amp; L(\hat y, y) &#x3D; - \sum _{i&#x3D;1}^{10,000} y_i log \hat y _i \end{matrix}[&#x2F;latex] the primary problem is computational speed, because of the softmax step is very expensive to calculate because needing to sum over your entire vocabulary size into the denominator of the softmax.</p>
<p><strong>a few solutions</strong></p>
<ul>
<li><ul>
<li>hierarchical softmax classifier</li>
<li>negative sampling</li>
</ul>
</li>
</ul>
<h4 id="CBow"><a href="#CBow" class="headerlink" title="CBow"></a>CBow</h4><p>the Continuous Bag-Of-Words Model, which takes the surrounding contexts from middle word, and and uses the surrounding words to try to predict the middle word.</p>
<h2 id="Negative-Sampling"><a href="#Negative-Sampling" class="headerlink" title="Negative Sampling"></a>Negative Sampling</h2><p>What to do in this algorithm is create a new supervised learning problem. And the problem is, given a pair of words like orange and juice, we’re going to predict is this a context-target pair? It’s really to <strong>try to distinguish between these two types of distributions</strong> from which you might sample a pair of words. <strong>How do you choose the negative examples?</strong></p>
<ul>
<li>sample the words in the middle, the candidate target words.</li>
<li>use 1 over the vocab size, sample the negative examples uniformly at random, but that’s also very non-representative of the distribution of English words.</li>
<li>the authors, Mikolov et al, reported that <strong>empirically</strong>, [latex]P(w_i) &#x3D; \frac{f(w_i)^{\frac{3}{4}}}{\sum _{j&#x3D;1}^{10,000}f(w_j)^{\frac{3}{4}}}[&#x2F;latex]</li>
</ul>
<h2 id="GloVe-Word-Vectors"><a href="#GloVe-Word-Vectors" class="headerlink" title="GloVe Word Vectors"></a>GloVe Word Vectors</h2><p><strong>GloVe</strong> stands for <strong>global vectors for word representation</strong>. Sampling pairs of words, context and target words, by picking two words that appear in close proximity to each other in our text corpus. So, what the GloVe algorithm does is, it starts off just by making that explicit. <img src="/img/A_note_on_the_featurization_view_of_word_embeddings.png"></p>
<h2 id="Sentiment-Classification"><a href="#Sentiment-Classification" class="headerlink" title="Sentiment Classification"></a>Sentiment Classification</h2><p>Sentiment classification is the task of looking at a piece of text and telling if someone <strong>likes or dislikes</strong> the thing they’re talking about. <img src="/img/RNN_for_sentiment_classification.png"></p>
<h2 id="Debiasing-Word-Embeddings"><a href="#Debiasing-Word-Embeddings" class="headerlink" title="Debiasing Word Embeddings"></a>Debiasing Word Embeddings</h2><p>Machine learning and AI algorithms are increasingly trusted to help with, or <strong>to make, extremely important decisions</strong>. And so we like to <strong>make sure that as much as possible that they’re free of undesirable forms of bias</strong>, such as gender bias, ethnicity bias and so on.</p>
<ul>
<li>So the first thing we’re going to do is <strong>identify the direction corresponding to a particular bias</strong> we want to reduce or eliminate.</li>
<li>the next step is a <strong>neutralization</strong> step. So for every word that’s not definitional, project it to get rid of bias.</li>
<li>And then the final step is called <strong>equalization</strong> in which you might have pairs of words such as grandmother and grandfather, or girl and boy, where you want the only difference in their embedding to be the gender.</li>
<li>And then, finally, the number of pairs you want to equalize, that’s <strong>actually also relatively small</strong>, and is, at least for the gender example, it is quite feasible to hand-pick.</li>
</ul>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2019/04/22/1-recurrent-neural-networks/" rel="prev" title="1 Recurrent Neural Networks">
      <i class="fa fa-chevron-left"></i> 1 Recurrent Neural Networks
    </a></div>
      <div class="post-nav-item">
    <a href="/2019/04/23/3-sequence-models-attention-mechanism/" rel="next" title="3 Sequence models & Attention mechanism">
      3 Sequence models & Attention mechanism <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Word-Representation"><span class="nav-number">1.</span> <span class="nav-text">Word Representation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Using-Word-Embeddings"><span class="nav-number">2.</span> <span class="nav-text">Using Word Embeddings</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Properties-of-Word-Embeddings"><span class="nav-number">3.</span> <span class="nav-text">Properties of Word Embeddings</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Embedding-Matrix"><span class="nav-number">4.</span> <span class="nav-text">Embedding Matrix</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Learning-Word-Embeddings"><span class="nav-number">5.</span> <span class="nav-text">Learning Word Embeddings</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Word2Vec"><span class="nav-number">6.</span> <span class="nav-text">Word2Vec</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Skip-Gram-model"><span class="nav-number">6.0.1.</span> <span class="nav-text">Skip-Gram model</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#CBow"><span class="nav-number">6.0.2.</span> <span class="nav-text">CBow</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Negative-Sampling"><span class="nav-number">7.</span> <span class="nav-text">Negative Sampling</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#GloVe-Word-Vectors"><span class="nav-number">8.</span> <span class="nav-text">GloVe Word Vectors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Sentiment-Classification"><span class="nav-number">9.</span> <span class="nav-text">Sentiment Classification</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Debiasing-Word-Embeddings"><span class="nav-number">10.</span> <span class="nav-text">Debiasing Word Embeddings</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Water"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Water</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">216</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">64</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/linmonsv" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;linmonsv" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:qin2@qq.com" title="E-Mail → mailto:qin2@qq.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2017 – 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Water</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
